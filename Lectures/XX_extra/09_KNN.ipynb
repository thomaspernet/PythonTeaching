{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"KNN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"m1u8ugOvcYMH","colab_type":"text"},"source":["# KNN\n","\n","[Source](https://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/)\n","\n","## What is k-Nearest Neighbors\n","\n","The model for kNN is the entire training dataset. When a prediction is required for a unseen data instance, the kNN algorithm will search through the training dataset for the k-most similar instances. The prediction attribute of the most similar instances is summarized and returned as the prediction for the unseen instance.\n","\n","The similarity measure is dependent on the type of data. For real-valued data, the Euclidean distance can be used. Other other types of data such as categorical or binary data, Hamming distance can be used.\n","\n","## How does k-Nearest Neighbors Work\n","\n","The kNN algorithm is belongs to the family of instance-based, competitive learning and lazy learning algorithms.\n","\n","Instance-based algorithms are those algorithms that model the problem using data instances (or rows) in order to make predictive decisions. The kNN algorithm is an extreme form of instance-based methods because all training observations are retained as part of the model.\n","\n","It is a competitive learning algorithm, because it internally uses competition between model elements (data instances) in order to make a predictive decision. The objective similarity measure between data instances causes each data instance to compete to “win” or be most similar to a given unseen data instance and contribute to a prediction.\n","\n","Lazy learning refers to the fact that the algorithm does not build a model until the time that a prediction is required. It is lazy because it only does work at the last second. This has the benefit of only including data relevant to the unseen data, called a localized model. A disadvantage is that it can be computationally expensive to repeat the same or similar searches over larger training datasets."]},{"cell_type":"code","metadata":{"id":"d_-SWDLGdtnX","colab_type":"code","colab":{}},"source":["from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pandas as pd\n","\n","iris = datasets.load_iris()\n","X = iris.data\n","y = iris.target\n","X_train, X_test, y_train, y_test = train_test_split(X,\n","                                                    y,\n","                                                    test_size=0.33,\n","                                                    random_state=42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BplEoUgElI3h","colab_type":"code","colab":{}},"source":["d_train = np.hstack([X_train, y_train.reshape((len(y_train), 1))])\n","d_test = np.hstack([X_test, y_test.reshape((len(y_test), 1))])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_GvrwQ6DjAzk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66},"outputId":"07641b2f-0351-4828-9c79-ee2b3bab7bdf","executionInfo":{"status":"ok","timestamp":1559457066527,"user_tz":-120,"elapsed":626,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}}},"source":["d_test[:3]"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[6.1, 2.8, 4.7, 1.2, 1. ],\n","       [5.7, 3.8, 1.7, 0.3, 0. ],\n","       [7.7, 2.6, 6.9, 2.3, 2. ]])"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"vuwZVU_7ePsF","colab_type":"text"},"source":["Step to compute Knn\n","\n","- Similarity: Calculate the distance between two data instances.\n","- Neighbors: Locate k most similar data instances.\n","  - Need to compute the distance column wise and then sort it\n","  - Then select the k most similar neighbors from the training set for a given test instance\n","- Response: Generate a response from a set of data instances.\n","- Accuracy: Summarize the accuracy of predictions.\n","- Main: Tie it all together.\n","\n","### SImilarity\n","\n","In order to make predictions we need to calculate the similarity between any two given data instances. This is needed so that we can locate the k most similar data instances in the training dataset for a given member of the test dataset and in turn make a prediction.\n","\n","$$\\begin{aligned} d(\\mathbf{p}, \\mathbf{q})=d(\\mathbf{q}, \\mathbf{p}) &=\\sqrt{\\left(q_{1}-p_{1}\\right)^{2}+\\left(q_{2}-p_{2}\\right)^{2}+\\cdots+\\left(q_{n}-p_{n}\\right)^{2}} \\\\ &=\\sqrt{\\sum_{i=1}^{n}\\left(q_{i}-p_{i}\\right)^{2}} \\end{aligned}$$\n"]},{"cell_type":"code","metadata":{"id":"K-_ut6-MesN4","colab_type":"code","colab":{}},"source":["def euclideanDistance(vect_1, vect_2):\n","  \n","  \"\"\"\n","  Compute the Euclidean distance \n","  \"\"\"\n","  \n","  sum_ = np.sum(np.power(vect_1 - vect_2, 2))\n","  \n","  euclideanD = np.sqrt(sum_)\n","  return euclideanD"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pxO-V9ggfUa3","colab_type":"code","outputId":"5b497c62-0522-4f11-f74c-9896477006d7","executionInfo":{"status":"ok","timestamp":1559457073645,"user_tz":-120,"elapsed":1009,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["vect1 = np.array([2, 3, 4])\n","vect2 = np.array([3,3,5])\n","\n","euclideanDistance(vect1, vect2)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.4142135623730951"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"pDsifz58f0-q","colab_type":"text"},"source":["### Neighbors\n","\n","Calculate the distance between test data and each row of training data.\n","\n","We want to sum each column of the test rows with all the columns of each rows in the train data\n","\n","Take an example, we want to compute the Euclidean distance of this test data:\n","- [7.2, 3.6, 5.1, 2.5]: It has 4 columns and one row\n","\n","we will compute the Euclidean distance with all the rows of the training dataset. The training dataset has also four columns. We are substracting each columns of the test and train dataset respecively \n","\n","Exemple with the first rows of the training data:\n","- [5.1, 3.5, 1.4, 0.2]\n","\n","The Euclidean distance is $$\\sqrt{((7.2 - 5.1)^2  + (3.6 - 3.5)^2 + (5.1 -1.4)^2 + (2.5 - 0.2)^2)}$$\n","\n","We repeat for all the rows in the training dataset\n","\n","When we have all the distances, we need to perform an ascending sorting\n","\n","Finaly, we extract the K neighbor. Note that if k = 1, we only select the closest neighbort\n"," "]},{"cell_type":"code","metadata":{"id":"pOk-tHusuGqI","colab_type":"code","colab":{}},"source":["def neighbors(train, test, k = 1):\n","  \"\"\"\n","  Compute the Euclidean distance with each rows of the train dataset\n","  Train contains the Y column at the last column of the array. Should \n","  not use it in the cmputation\n","  \"\"\"\n","  \n","  columns_x = train[:, :-1]\n","  \n","  n_rows = len(columns_x)\n","  list_distance = []\n","  list_label = []\n","  \n","  for i in range(0, n_rows):\n","    \n","    distance = euclideanDistance(columns_x[i], test)\n","    \n","    ### Store the distance\n","    list_distance.append(distance)\n","    \n","    ### Store the label from the training data\n","    list_label.append(train[i,-1])\n","    \n","  dic_distance_label = {\n","      'distances':list_distance,\n","      'labels' : list_label\n","  }\n","  \n","  #### Get a DataFrame, easier to visualize\n","  \n","  EuclideanDis = pd.DataFrame(dic_distance_label).sort_values('distances',\n","                                                              ascending = True)\n","  \n","  #### Extract the k neighbor\n","  \n","  k_neighbors = EuclideanDis.head(k)\n","    \n","  return k_neighbors"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xse91bcY9Ul-","colab_type":"text"},"source":["Let's make a test with the first row of the test array.\n","\n","The test array has the following values:\n","\n","- [6.1, 2.8, 4.7, 1.2]\n","- class:  1"]},{"cell_type":"code","metadata":{"id":"gqaXqOjJvVOO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":76},"outputId":"b38ebca6-f486-4e92-b26e-f2f11ac89c44","executionInfo":{"status":"ok","timestamp":1559457093296,"user_tz":-120,"elapsed":552,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}}},"source":["vect1 = d_test[:1,:-1]\n","\n","k_neighbors = neighbors(train = d_train, test = vect1, k = 1)\n","k_neighbors"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>distances</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>59</th>\n","      <td>0.223607</td>\n","      <td>1.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    distances  labels\n","59   0.223607     1.0"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"5fUsD1LJpqLI","colab_type":"text"},"source":["## Responce\n","\n","We want to get the most frequent class of k distances return in the previous steps. \n","\n","We can do this by allowing each neighbor to vote for their class attribute, and take the majority vote as the prediction.\n","\n","Since we return a pandas Dataframe in the `neighbors` function, we can jointly us the build-in `groupby` with `count`\n","to get the majority"]},{"cell_type":"code","metadata":{"id":"F1ZE3CDBntDm","colab_type":"code","colab":{}},"source":["def responce(k_neighbors):\n","  \"\"\"\n","  We need to count the number of occurences for each class. \n","  \n","  The one with the most vote win, and be declared the predicted class.\n","  \n","  Not that, if k = 1, then obviously no vote needed.\n","  \n","  We also want to return the index of the nearest rows in the training set\n","\n","  \"\"\"\n","  \n","  groupby = k_neighbors.groupby('labels').aggregate(\n","      {'distances': 'count'}).sort_values('distances',\n","                                          ascending = False)\n","  \n","  groupby = groupby.rename(index=str, columns={\"distances\": \"Count\"})\n","  \n","  majority = groupby.reset_index().head(1)\n","  winner = float(majority['labels'][0])\n","  #### \n","  index_winner = k_neighbors[k_neighbors['labels'] == winner].index.tolist()\n","  \n","  dic_final = {\n","      \n","      'class_winner': winner,\n","      'index_train': index_winner\n","  }\n","  \n","  return dic_final\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0mMO5zUotKBK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"c9ee93e9-2c69-4c0a-aead-0b27f64b0d6a","executionInfo":{"status":"ok","timestamp":1559457980999,"user_tz":-120,"elapsed":536,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}}},"source":["responce(k_neighbors = k_neighbors)"],"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'class_winner': 1.0, 'index_train': [59]}"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"markdown","metadata":{"id":"JOqHJI0h8SKK","colab_type":"text"},"source":["## Wrap algorithm"]},{"cell_type":"code","metadata":{"id":"kPPjQWiR8TyL","colab_type":"code","colab":{}},"source":["def knn(train, test, k = 1):\n","  \"\"\"\n","  Wrap two functions to get the predicted class\n","  \n","  1: neighbors: Compute Euclidean distance columns wise between\n","  train and test set\n","  Ascending sort and extract k nearest neighbors\n","  2: responce: Compute the winning class as the majority by label\n","  \n","  return the winning class and the index in the train set\n","  \"\"\"\n","  \n","  k_neighbors = neighbors(train = train, test = test, k = k)\n","  \n","  winner = responce(k_neighbors = k_neighbors)\n","  \n","  return winner"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"72wWY8jO9G_U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"d622c26c-ba21-4359-893e-6f18887b30a4","executionInfo":{"status":"ok","timestamp":1559458418766,"user_tz":-120,"elapsed":510,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}}},"source":["knn(train = d_train, test = vect1, k = 3)"],"execution_count":75,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'class_winner': 1.0, 'index_train': [59, 70, 19]}"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"markdown","metadata":{"id":"VkSj7vTGvk5S","colab_type":"text"},"source":["## Test Scikit learn\n","\n","We can compare the results with Scikit learn"]},{"cell_type":"code","metadata":{"id":"MEU5BsGGvm-p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"9153fe62-5f4c-4891-fa98-3e2be024f621","executionInfo":{"status":"ok","timestamp":1559458420980,"user_tz":-120,"elapsed":769,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}}},"source":["from sklearn.neighbors import KNeighborsClassifier\n","neigh = KNeighborsClassifier(n_neighbors=3)\n","\n","neigh.fit(X_train, y_train)\n","\n","print(neigh.predict(vect1.reshape((1,4))),\n","neigh.kneighbors(vect1.reshape((1,4)))[1])"],"execution_count":76,"outputs":[{"output_type":"stream","text":["[1] [[59 70 19]]\n"],"name":"stdout"}]}]}