{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"05_stepwise_regression.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["7ngxhxHhHH1Q","cTsJ7NltXkiy"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LXRnhElACMKB","colab_type":"text"},"source":["# Stepwise Regression\n","\n","From [STAT 501](https://newonlinecourses.science.psu.edu/stat501/node/329/)\n","\n","In this section, we learn about the stepwise regression procedure. While we will soon learn the finer details, the general idea behind the stepwise regression procedure is that we build our regression model from a set of candidate predictor variables by entering and removing predictors — in a stepwise manner — into our model until there is no justifiable reason to enter or remove any more\n"]},{"cell_type":"markdown","metadata":{"id":"LI7XCKzjClhI","colab_type":"text"},"source":["## An example\n","\n","Let's learn how the stepwise regression procedure works by considering a data set that concerns the hardening of cement. \n","\n","In particular, the researchers were interested in learning how the composition of the cement affected the heat evolved during the hardening of the cement\n","\n","Therefore, they measured and recorded the following data on 13 batches of cement:\n","\n","- Response y: heat evolved in calories during hardening of cement on a per gram basis\n","- Predictor x1: % of tricalcium aluminate\n","- Predictor x2: % of tricalcium silicate\n","- Predictor x3: % of tetracalcium alumino ferrite\n","- Predictor x4: % of dicalcium silicate"]},{"cell_type":"markdown","metadata":{"id":"P9hVYGO_DhRG","colab_type":"text"},"source":["### Upload data"]},{"cell_type":"code","metadata":{"id":"ILX0fewmBYNU","colab_type":"code","outputId":"1ded8b58-f8fa-44d2-de92-11ee7522abe5","executionInfo":{"status":"ok","timestamp":1557240621432,"user_tz":-120,"elapsed":1181,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":82}},"source":["from GoogleDrivePy.google_authentification import connect_service_local\n","from GoogleDrivePy.google_drive import connect_drive\n","import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import statsmodels.api as sm\n","pathcredential = '/Users/Thomas/Google Drive/Projects/Data_science/Google_code_n_Oauth/Client_Oauth/Google_auth/'\n","scopes = ['https://www.googleapis.com/auth/documents.readonly',\n","            'https://www.googleapis.com/auth/drive', \n","         'https://www.googleapis.com/auth/spreadsheets.readonly']\n","serviceaccount = '/Users/Thomas/Google Drive/Projects/Data_science/Google_code_n_Oauth/Client_Oauth/Google_auth/valid-pagoda-132423-c6ac84b41833.json'\n","cs = connect_service_local.connect_service_local(path_json =pathcredential,\n","                                                 path_service_account = serviceaccount,\n","                                                 scope = scopes)\n","service = cs.get_service()\n","\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Service Google Drive and Docs, Sheet are now connected. \n","Service Google Drive is stored as <googleapiclient.discovery.Resource object at 0x1a1ab56860> and accessible with \"drive\" \n","Service Google Doc is stored as <googleapiclient.discovery.Resource object at 0x1c1fb5f1d0> and accessible with \"doc\" \n","Service Google Sheet is stored as <googleapiclient.discovery.Resource object at 0x1c1fc40dd8>and accessible with \"sheet\"\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gSj10zQUw4zg","colab_type":"code","colab":{}},"source":["headers = ['y','x1','x2','x3','x4']\n","range_name = 'cement.csv!A2:E14'\n","\n","df = service['sheet'].spreadsheets().values().get(\n","    spreadsheetId='1QvLOtbVBOaeDpDzg5gtrarIlcKgorC1qgVbqpNyIlCQ',\n","    range=range_name).execute()\n","df = pd.DataFrame(df.get('values', []), columns=headers)\n","df = df.apply(pd.to_numeric, errors='ignore')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecdW-08RERD-","colab_type":"text"},"source":["### Scatter plot matrix\n","\n","you can get a hunch of which predictors are good candidates for being the first to enter the stepwise model. \n","\n","It looks as if the strongest relationship exists between either y and x2 or between y and x4 — and therefore, perhaps either x2 or x4 should enter the stepwise model first.\n","\n","Did you notice what else is going on in this data set though? A strong correlation also exists between the predictors x2 and x4! \n","\n","How does this correlation among the predictor variables play out in the stepwise procedure? Let's see what happens when we use the stepwise regression method to find a model that is appropriate for these data\n","\n","**Note**. The number of predictors in this data set is not large. The stepwise procedure is typically used on much larger data sets, for which it is not feasible to attempt to fit all of the possible regression models. For the sake of illustration, the data set here is necessarily small, so that the largeness of the data set does not obscure the pedagogical point being made."]},{"cell_type":"code","metadata":{"id":"kR-LhJ9UETHp","colab_type":"code","outputId":"cd26a085-b052-4a06-dac3-cfaafd6cd10e","executionInfo":{"status":"ok","timestamp":1557240610656,"user_tz":-120,"elapsed":2679,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["import seaborn as sns\n","sns.set(style=\"ticks\")\n","\n","sns.pairplot(df)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<seaborn.axisgrid.PairGrid at 0x10f3a04e0>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"vDh58Ki1Elsg","colab_type":"text"},"source":["## The procedure\n","\n","First, we start with no predictors in our \"stepwise model.\" Then, at each step along the way we either enter or remove a predictor based on the partial F-tests — that is, the t-tests for the slope parameters — that are obtained. \n","\n","We stop when no more predictors can be justifiably entered or removed from our stepwise model, thereby leading us to a \"final model.\"\n","\n","Starting the procedure. The first thing we need to do is set a significance level for deciding when to enter a predictor into the stepwise model. \n","\n","We'll call this the Alpha-to-Enter significance level and will denote it as $\\alpha_{\\mathrm{E}}$. Of course, we also need to set a significance level for deciding when to remove a predictor from the stepwise model. \n","\n","We'll call this the Alpha-to-Remove significance level and will denote it as $\\alpha_{\\mathrm{R}}$.\n","\n","- Specify an Alpha-to-Enter significance level. This will typically be greater than the usual 0.05 level so that it is not too difficult to enter predictors into the model. \n","- Specify an Alpha-to-Remove significance level. This will typically be greater than the usual 0.05 level so that it is not too easy to remove predictors from the model.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"clSMHZjZFJW-","colab_type":"text"},"source":["### Step 1\n","\n","Once we've specified the starting significance levels, then we:\n","\n","1. Fit each of the one-predictor models — that is, regress y on x1, regress y on x2, ..., and regress y on xp-1.\n","2. Of those predictors whose t-test P-value is less than αE = 0.15, the first predictor put in the stepwise model is the predictor that has the smallest t-test P-value.\n","3. If no predictor has a t-test P-value less than $\\alpha_{\\mathrm{E}} = 0.15$, stop."]},{"cell_type":"code","metadata":{"id":"xB5vU-npFQIW","colab_type":"code","colab":{}},"source":["import statsmodels.api as sm\n","\n","list_x = ['x1','x2','x3','x4']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGZqecIeFucR","colab_type":"code","colab":{}},"source":["y = df['y'].to_numpy()\n","dic_stepwise = {\n","    'var': [],\n","    'p_value': []\n","}\n","#dic_stepwise['var'].update('X1')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RkoDZWyGxy8","colab_type":"code","outputId":"b7d08bb2-21b0-4ff8-8898-7c8a6ca11e34","executionInfo":{"status":"ok","timestamp":1556547280140,"user_tz":-120,"elapsed":53,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":98}},"source":["p_value_final = 0.15\n","var_final = ''\n","for var in list_x:\n","  x = df[var].to_numpy()\n","  x = sm.add_constant(x)\n","  model = sm.OLS(y,x).fit()  \n","  p_value = model.pvalues[1]\n","  print(\"p_value: %f\" % (p_value))\n","  if p_value < p_value_final:\n","    p_value_final = p_value\n","    var_final =  var\n","\n","dic_stepwise['var'].append(var_final)\n","dic_stepwise['p_value'].append(p_value_final)\n","dic_stepwise"],"execution_count":0,"outputs":[{"output_type":"stream","text":["p_value: 0.004552\n","p_value: 0.000665\n","p_value: 0.059762\n","p_value: 0.000576\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'var': ['x4'], 'p_value': [0.0005762318164885003]}"]},"metadata":{"tags":[]},"execution_count":233}]},{"cell_type":"markdown","metadata":{"id":"7ngxhxHhHH1Q","colab_type":"text"},"source":["### Step 2\n","\n","1.  $x4$ has the smallest t-test P-value below $\\alpha_{\\mathrm{E}}=0.15$ and therefore was deemed the \"best\" single predictor arising from the the first step.\n","2. Now, fit each of the two-predictor models that include $x4$ as a predictor — that is, regress $y$ on $x4$ and $x2$, regress $y$ on $x4$ and $x3$, ..., and regress $y$ on$x1$ and $xp-1$.\n","3. Of those predictors whose t-test P-value is less than $\\alpha_{\\mathrm{E}}=0.15$, the second predictor put in the stepwise model is the predictor that has the smallest t-test P-value\n","4. If no predictor has a t-test P-value less than $\\alpha_{\\mathrm{E}}=0.15$, stop. The model with the one predictor obtained from the first step is your final model."]},{"cell_type":"code","metadata":{"id":"jeiQZDb_Hz6D","colab_type":"code","outputId":"c5b404e1-3187-4e74-a782-fcb6715acbe3","executionInfo":{"status":"ok","timestamp":1556547281638,"user_tz":-120,"elapsed":45,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":82}},"source":["### Forward\n","p_value_final = 0.15\n","var_final = ''\n","\n","for var in list_x:\n","  if var not in dic_stepwise['var']:\n","    \n","    list_temps = []\n","    for value in dic_stepwise['var']:\n","      list_temps.append(value)\n","    list_temps.append(var)\n","    \n","    x = df[list_temps].to_numpy()\n","    x = sm.add_constant(x)\n","    model = sm.OLS(y,x).fit()\n","    p_value = model.pvalues[2]\n","    \n","    print(\"p_value:\" +  str(p_value))\n","    \n","    \n","    if p_value < p_value_final:\n","      p_value_final = p_value\n","      var_final =  var\n","print(var_final, p_value_final)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["p_value:1.1052814195373175e-06\n","p_value:0.6866842279633266\n","p_value:8.375467286314457e-05\n","x1 1.1052814195373175e-06\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pO3HgBRYPlIM","colab_type":"text"},"source":["5. But, suppose instead that x1 was deemed the \"best\" second predictor and it is therefore entered into the stepwise model.\n","6. Now, since $x4$ was the first predictor in the model, step back and see if entering $x1$ into the stepwise model somehow affected the significance of the $x4$ predictor. That is, check the t-test P-value for testing $\\beta_{1}=0$. If the t-test P-value for\\beta_{1}=0 has become not significant — that is, the P-value is greater than $\\alpha_{\\mathrm{R}}=0.15$ — remove $x4$ from the stepwise model."]},{"cell_type":"code","metadata":{"id":"WGP8SHxAJ299","colab_type":"code","outputId":"2269ed6c-af65-4bd0-b468-db1142958326","executionInfo":{"status":"ok","timestamp":1556547283204,"user_tz":-120,"elapsed":45,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":66}},"source":["  ### backward\n","if var_final != '':\n","  \n","  list_temps = []\n","  for value in dic_stepwise['var']:\n","    list_temps.append(value)\n","  list_temps.append(var_final)\n","    \n","    \n","  x = df[list_temps].to_numpy()\n","  x = sm.add_constant(x)\n","  model = sm.OLS(y,x).fit()\n","  p_value = model.pvalues[1]\n","  print(\"p_value:\" +  str(p_value))\n","  \n","  \n","  if p_value < 0.15:\n","    dic_stepwise['var'].append(var_final)\n","    dic_stepwise['p_value'].append(p_value_final)\n","  else:\n","    dic_stepwise['var'][0] = var_final\n","    dic_stepwise['p_value'][1] = p_value_final\n","dic_stepwise "],"execution_count":0,"outputs":[{"output_type":"stream","text":["p_value:1.814890465259972e-07\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["{'var': ['x4', 'x1'],\n"," 'p_value': [0.0005762318164885003, 1.1052814195373175e-06]}"]},"metadata":{"tags":[]},"execution_count":235}]},{"cell_type":"markdown","metadata":{"id":"cTsJ7NltXkiy","colab_type":"text"},"source":["### Step 3\n","\n","1. Suppose both $x4$ and $x1$ made it into the two-predictor stepwise model and remained there.\n","\n","2. Now, fit each of the three-predictor models that include $x4$ and $x1$ as predictors — that is, regress $y$ on $x4$, $x1$, and $x2$, regress $y$ on $x4$, $x1$, and $x3$, ..., and regress $y$ on $x4$, $x1$, and $xp-1$.\n","3. Of those predictors whose t-test P-value is less than $\\alpha_{\\mathrm{E}}=0.15$, the third predictor put in the stepwise model is the predictor that has the smallest t-test P-value.\n","4. If no predictor has a t-test P-value less than $\\alpha_{\\mathrm{E}}=0.15$, stop. The model containing the two predictors obtained from the second step is your final model."]},{"cell_type":"code","metadata":{"id":"t3iUwSUlYJwo","colab_type":"code","outputId":"c143a0dd-0b46-4519-a83f-58211bf22d77","executionInfo":{"status":"ok","timestamp":1556547285608,"user_tz":-120,"elapsed":39,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":66}},"source":["### Forward\n","p_value_final = 0.15\n","var_final = ''\n","\n","for var in list_x:\n","  if var not in dic_stepwise['var']:\n","    list_temps = []\n","    for value in dic_stepwise['var']:\n","      list_temps.append(value)\n","    list_temps.append(var)\n","    \n","    x = df[list_temps].to_numpy()\n","    x = sm.add_constant(x)\n","    model = sm.OLS(y,x).fit()\n","    p_value = model.pvalues[2]\n","    print(\"p_value:\" +  str(p_value))\n","    \n","    if p_value < p_value_final:\n","      p_value_final = p_value\n","      var_final =  var\n","print(var_final, p_value_final)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["p_value:5.780763673513142e-07\n","p_value:0.0011163909896500272\n","x2 5.780763673513142e-07\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1CdV6sXuY6DN","colab_type":"text"},"source":["5. But, instead that $x2$ was deemed the \"best\" third predictor and it is therefore entered into the stepwise model.\n","6. Now, since $x4$ and $x1$ were the first predictors in the model, step back and see if entering $x2$ into the stepwise model somehow affected the significance of the $x4$ and $x1$ predictors. That is, check the t-test P-values for testing $\\beta_{1}=0$ and $\\beta_{2}=0$. If the t-test P-value for either $\\beta_{1}=0$or $\\beta_{2}=0$ has become not significant — that is, the P-value is greater than $\\alpha_{\\mathrm{R}}=0.15$— remove the predictor from the stepwise model."]},{"cell_type":"code","metadata":{"id":"5In7z3pGZVZw","colab_type":"code","outputId":"a0f55b6c-9182-4add-a6e1-f12eb1385397","executionInfo":{"status":"ok","timestamp":1556547287299,"user_tz":-120,"elapsed":48,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":98}},"source":["  ### backward\n","if var_final != '':\n","  \n","  list_temps = []\n","  for value in dic_stepwise['var']:\n","    list_temps.append(value)\n","  list_temps.append(var_final)\n","    \n","    \n","  x = df[list_temps].to_numpy()\n","  x = sm.add_constant(x)\n","  model = sm.OLS(y,x).fit()\n","  size_pvalue = len(model.pvalues)\n","  \n","  for i, value in enumerate(model.pvalues):\n","    if (i != size_pvalue) and (value > 0.15):\n","      print(\"p_value:\" +  str(value) + \" index \" + str(i - 1))\n","      dic_stepwise['var'].pop(i-1)\n","      dic_stepwise['p_value'].pop(i - 1)\n","    elif (i == size_pvalue - 1) and (value  < 0.15):\n","      dic_stepwise['var'].append(var_final)\n","      dic_stepwise['p_value'].append(p_value_final)\n","      \n","dic_stepwise"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 0.0006753321641596413\n","1 0.20539543810168748\n","p_value:0.20539543810168748 index 0\n","2 5.780763673513142e-07\n","3 0.05168734897742293\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MhwlcWYxc3YS","colab_type":"text"},"source":["### Repeat\n","\n","Stopping the procedure. Continue the steps as described above until adding an additional predictor does not yield a t-test P-value below $\\alpha_{\\mathrm{E}}=0.15$"]},{"cell_type":"code","metadata":{"id":"ggbfyMfVYjXE","colab_type":"code","colab":{}},"source":["def stepwise_regression(df, X, y):\n","  y = df[y].to_numpy()\n","  dic_stepwise = {\n","    'var': [],\n","    'p_value': []\n","  }\n","\n","  list_x = X\n","\n","  p_value_final = 0.15\n","  var_final = ''\n","  worst_pvalue = 0\n","  #### Forward\n","  while worst_pvalue < 0.15:\n","  #changed = False\n","    p_value_final = 0.15\n","    var_final = ''\n","    for var in list_x:\n","    \n","      if var not in dic_stepwise['var']:\n","        list_temps = []\n","        for value in dic_stepwise['var']:\n","          list_temps.append(value)\n","        list_temps.append(var)\n","        x = df[list_temps].to_numpy()\n","        x = sm.add_constant(x)\n","        model = sm.OLS(y,x).fit()\n","    \n","        size_pvalue = len(model.pvalues)\n","        for i, value in enumerate(model.pvalues):\n","          if (i == size_pvalue - 1):\n","            p_value = model.pvalues[i]\n","      #print(\"p_value:\" +  str(p_value))\n","    \n","        if p_value < p_value_final:\n","          p_value_final = p_value\n","          var_final =  var\n","        #changed = True\n","        \n","  #print(var_final)\n","\n","  #### Backaward\n","    if var_final != '':\n","  \n","      list_temps = []\n","      for value in dic_stepwise['var']:\n","        list_temps.append(value)\n","      list_temps.append(var_final)\n","      print(list_temps)\n","      x = df[list_temps].to_numpy()\n","      x = sm.add_constant(x)\n","      model = sm.OLS(y,x).fit()\n","      size_pvalue = len(model.pvalues)\n","    \n","      for i, value in enumerate(model.pvalues):\n","        if (i != size_pvalue -1) and (value > 0.15):\n","          print(\"p_value:\" +  str(value) + \" index \" + str(i - 1))\n","          dic_stepwise['var'].pop(i-1)\n","          dic_stepwise['p_value'].pop(i - 1)\n","        elif (i == size_pvalue - 1) and (value  < 0.15):\n","          dic_stepwise['var'].append(var_final)\n","          dic_stepwise['p_value'].append(p_value_final)\n","    \n","    worst_pvalue =  model.pvalues[1:].max()\n","    print(worst_pvalue)\n","    print(dic_stepwise['var'])\n","  \n","    \n","      \n","      \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zcC7DCHlv1vO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":180},"outputId":"d0fba8f8-a723-409f-8ed1-8169ad3ddabb","executionInfo":{"status":"ok","timestamp":1557240627767,"user_tz":-120,"elapsed":97,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}}},"source":["stepwise_regression(df, X = ['x1', 'x2', 'x4'], y = 'y')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["['x4']\n","0.0005762318164885003\n","['x4']\n","['x4', 'x1']\n","1.1052814195373175e-06\n","['x4', 'x1']\n","['x4', 'x1', 'x2']\n","p_value:0.20539543810168748 index 0\n","0.20539543810168748\n","['x1', 'x2']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B9Jt5aKNPGYE","colab_type":"code","colab":{}},"source":["df[['x1', 'x2', 'x4']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXgY-v-wP9Jb","colab_type":"code","outputId":"8da54d18-ae37-4053-f0f3-837252977c15","executionInfo":{"status":"ok","timestamp":1556560740066,"user_tz":-120,"elapsed":54,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":552}},"source":["x = df[['x1', 'x2','x3', 'x4']]\n","x = sm.add_constant(x)\n","sm.OLS(y,x).fit().summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/Users/Thomas/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n","  return getattr(obj, method)(*args, **kwds)\n","/Users/Thomas/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1394: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=13\n","  \"anyway, n=%i\" % int(n))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.982\n","Model:                            OLS   Adj. R-squared:                  0.974\n","Method:                 Least Squares   F-statistic:                     111.5\n","Date:                Mon, 29 Apr 2019   Prob (F-statistic):           4.76e-07\n","Time:                        19:59:00   Log-Likelihood:                -26.918\n","No. Observations:                  13   AIC:                             63.84\n","Df Residuals:                       8   BIC:                             66.66\n","Df Model:                           4                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const         62.4054     70.071      0.891      0.399     -99.179     223.989\n","x1             1.5511      0.745      2.083      0.071      -0.166       3.269\n","x2             0.5102      0.724      0.705      0.501      -1.159       2.179\n","x3             0.1019      0.755      0.135      0.896      -1.638       1.842\n","x4            -0.1441      0.709     -0.203      0.844      -1.779       1.491\n","==============================================================================\n","Omnibus:                        0.165   Durbin-Watson:                   2.053\n","Prob(Omnibus):                  0.921   Jarque-Bera (JB):                0.320\n","Skew:                           0.201   Prob(JB):                        0.852\n","Kurtosis:                       2.345   Cond. No.                     6.06e+03\n","==============================================================================\n","\n","Warnings:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 6.06e+03. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n","\"\"\""],"text/html":["<table class=\"simpletable\">\n","<caption>OLS Regression Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.982</td>\n","</tr>\n","<tr>\n","  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.974</td>\n","</tr>\n","<tr>\n","  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   111.5</td>\n","</tr>\n","<tr>\n","  <th>Date:</th>             <td>Mon, 29 Apr 2019</td> <th>  Prob (F-statistic):</th> <td>4.76e-07</td>\n","</tr>\n","<tr>\n","  <th>Time:</th>                 <td>19:59:00</td>     <th>  Log-Likelihood:    </th> <td> -26.918</td>\n","</tr>\n","<tr>\n","  <th>No. Observations:</th>      <td>    13</td>      <th>  AIC:               </th> <td>   63.84</td>\n","</tr>\n","<tr>\n","  <th>Df Residuals:</th>          <td>     8</td>      <th>  BIC:               </th> <td>   66.66</td>\n","</tr>\n","<tr>\n","  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>   \n","</tr>\n","<tr>\n","  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n","</tr>\n","<tr>\n","  <th>const</th> <td>   62.4054</td> <td>   70.071</td> <td>    0.891</td> <td> 0.399</td> <td>  -99.179</td> <td>  223.989</td>\n","</tr>\n","<tr>\n","  <th>x1</th>    <td>    1.5511</td> <td>    0.745</td> <td>    2.083</td> <td> 0.071</td> <td>   -0.166</td> <td>    3.269</td>\n","</tr>\n","<tr>\n","  <th>x2</th>    <td>    0.5102</td> <td>    0.724</td> <td>    0.705</td> <td> 0.501</td> <td>   -1.159</td> <td>    2.179</td>\n","</tr>\n","<tr>\n","  <th>x3</th>    <td>    0.1019</td> <td>    0.755</td> <td>    0.135</td> <td> 0.896</td> <td>   -1.638</td> <td>    1.842</td>\n","</tr>\n","<tr>\n","  <th>x4</th>    <td>   -0.1441</td> <td>    0.709</td> <td>   -0.203</td> <td> 0.844</td> <td>   -1.779</td> <td>    1.491</td>\n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","  <th>Omnibus:</th>       <td> 0.165</td> <th>  Durbin-Watson:     </th> <td>   2.053</td>\n","</tr>\n","<tr>\n","  <th>Prob(Omnibus):</th> <td> 0.921</td> <th>  Jarque-Bera (JB):  </th> <td>   0.320</td>\n","</tr>\n","<tr>\n","  <th>Skew:</th>          <td> 0.201</td> <th>  Prob(JB):          </th> <td>   0.852</td>\n","</tr>\n","<tr>\n","  <th>Kurtosis:</th>      <td> 2.345</td> <th>  Cond. No.          </th> <td>6.06e+03</td>\n","</tr>\n","</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 6.06e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."]},"metadata":{"tags":[]},"execution_count":319}]},{"cell_type":"code","metadata":{"id":"eNhzPF06Pa3I","colab_type":"code","outputId":"bfab7c78-54a2-4fcc-f912-5707c5abce7a","executionInfo":{"status":"ok","timestamp":1556560594310,"user_tz":-120,"elapsed":56,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":485}},"source":["x = df[['x1', 'x2']]\n","x = sm.add_constant(x)\n","sm.OLS(y,x).fit().summary()\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["/Users/Thomas/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n","  return getattr(obj, method)(*args, **kwds)\n","/Users/Thomas/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1394: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=13\n","  \"anyway, n=%i\" % int(n))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.979\n","Model:                            OLS   Adj. R-squared:                  0.974\n","Method:                 Least Squares   F-statistic:                     229.5\n","Date:                Mon, 29 Apr 2019   Prob (F-statistic):           4.41e-09\n","Time:                        19:56:34   Log-Likelihood:                -28.156\n","No. Observations:                  13   AIC:                             62.31\n","Df Residuals:                      10   BIC:                             64.01\n","Df Model:                           2                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const         52.5773      2.286     22.998      0.000      47.483      57.671\n","x1             1.4683      0.121     12.105      0.000       1.198       1.739\n","x2             0.6623      0.046     14.442      0.000       0.560       0.764\n","==============================================================================\n","Omnibus:                        1.509   Durbin-Watson:                   1.922\n","Prob(Omnibus):                  0.470   Jarque-Bera (JB):                1.104\n","Skew:                           0.503   Prob(JB):                        0.576\n","Kurtosis:                       1.987   Cond. No.                         175.\n","==============================================================================\n","\n","Warnings:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","\"\"\""],"text/html":["<table class=\"simpletable\">\n","<caption>OLS Regression Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.979</td>\n","</tr>\n","<tr>\n","  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.974</td>\n","</tr>\n","<tr>\n","  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   229.5</td>\n","</tr>\n","<tr>\n","  <th>Date:</th>             <td>Mon, 29 Apr 2019</td> <th>  Prob (F-statistic):</th> <td>4.41e-09</td>\n","</tr>\n","<tr>\n","  <th>Time:</th>                 <td>19:56:34</td>     <th>  Log-Likelihood:    </th> <td> -28.156</td>\n","</tr>\n","<tr>\n","  <th>No. Observations:</th>      <td>    13</td>      <th>  AIC:               </th> <td>   62.31</td>\n","</tr>\n","<tr>\n","  <th>Df Residuals:</th>          <td>    10</td>      <th>  BIC:               </th> <td>   64.01</td>\n","</tr>\n","<tr>\n","  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n","</tr>\n","<tr>\n","  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n","</tr>\n","<tr>\n","  <th>const</th> <td>   52.5773</td> <td>    2.286</td> <td>   22.998</td> <td> 0.000</td> <td>   47.483</td> <td>   57.671</td>\n","</tr>\n","<tr>\n","  <th>x1</th>    <td>    1.4683</td> <td>    0.121</td> <td>   12.105</td> <td> 0.000</td> <td>    1.198</td> <td>    1.739</td>\n","</tr>\n","<tr>\n","  <th>x2</th>    <td>    0.6623</td> <td>    0.046</td> <td>   14.442</td> <td> 0.000</td> <td>    0.560</td> <td>    0.764</td>\n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","  <th>Omnibus:</th>       <td> 1.509</td> <th>  Durbin-Watson:     </th> <td>   1.922</td>\n","</tr>\n","<tr>\n","  <th>Prob(Omnibus):</th> <td> 0.470</td> <th>  Jarque-Bera (JB):  </th> <td>   1.104</td>\n","</tr>\n","<tr>\n","  <th>Skew:</th>          <td> 0.503</td> <th>  Prob(JB):          </th> <td>   0.576</td>\n","</tr>\n","<tr>\n","  <th>Kurtosis:</th>      <td> 1.987</td> <th>  Cond. No.          </th> <td>    175.</td>\n","</tr>\n","</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."]},"metadata":{"tags":[]},"execution_count":318}]},{"cell_type":"code","metadata":{"id":"LJShj0yEOu_S","colab_type":"code","outputId":"7a2049fc-e082-45f9-d888-bd7f50c087aa","executionInfo":{"status":"ok","timestamp":1556560564849,"user_tz":-120,"elapsed":66,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":502}},"source":["x = df[['x1', 'x2', 'x3']]\n","x = sm.add_constant(x)\n","sm.OLS(y,x).fit().summary()\n","  "],"execution_count":0,"outputs":[{"output_type":"stream","text":["/Users/Thomas/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n","  return getattr(obj, method)(*args, **kwds)\n","/Users/Thomas/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1394: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=13\n","  \"anyway, n=%i\" % int(n))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.982\n","Model:                            OLS   Adj. R-squared:                  0.976\n","Method:                 Least Squares   F-statistic:                     166.3\n","Date:                Mon, 29 Apr 2019   Prob (F-statistic):           3.37e-08\n","Time:                        19:56:04   Log-Likelihood:                -26.952\n","No. Observations:                  13   AIC:                             61.90\n","Df Residuals:                       9   BIC:                             64.16\n","Df Model:                           3                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const         48.1936      3.913     12.315      0.000      39.341      57.046\n","x1             1.6959      0.205      8.290      0.000       1.233       2.159\n","x2             0.6569      0.044     14.851      0.000       0.557       0.757\n","x3             0.2500      0.185      1.354      0.209      -0.168       0.668\n","==============================================================================\n","Omnibus:                        0.164   Durbin-Watson:                   2.110\n","Prob(Omnibus):                  0.921   Jarque-Bera (JB):                0.261\n","Skew:                           0.208   Prob(JB):                        0.878\n","Kurtosis:                       2.445   Cond. No.                         319.\n","==============================================================================\n","\n","Warnings:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","\"\"\""],"text/html":["<table class=\"simpletable\">\n","<caption>OLS Regression Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.982</td>\n","</tr>\n","<tr>\n","  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.976</td>\n","</tr>\n","<tr>\n","  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   166.3</td>\n","</tr>\n","<tr>\n","  <th>Date:</th>             <td>Mon, 29 Apr 2019</td> <th>  Prob (F-statistic):</th> <td>3.37e-08</td>\n","</tr>\n","<tr>\n","  <th>Time:</th>                 <td>19:56:04</td>     <th>  Log-Likelihood:    </th> <td> -26.952</td>\n","</tr>\n","<tr>\n","  <th>No. Observations:</th>      <td>    13</td>      <th>  AIC:               </th> <td>   61.90</td>\n","</tr>\n","<tr>\n","  <th>Df Residuals:</th>          <td>     9</td>      <th>  BIC:               </th> <td>   64.16</td>\n","</tr>\n","<tr>\n","  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n","</tr>\n","<tr>\n","  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n","</tr>\n","<tr>\n","  <th>const</th> <td>   48.1936</td> <td>    3.913</td> <td>   12.315</td> <td> 0.000</td> <td>   39.341</td> <td>   57.046</td>\n","</tr>\n","<tr>\n","  <th>x1</th>    <td>    1.6959</td> <td>    0.205</td> <td>    8.290</td> <td> 0.000</td> <td>    1.233</td> <td>    2.159</td>\n","</tr>\n","<tr>\n","  <th>x2</th>    <td>    0.6569</td> <td>    0.044</td> <td>   14.851</td> <td> 0.000</td> <td>    0.557</td> <td>    0.757</td>\n","</tr>\n","<tr>\n","  <th>x3</th>    <td>    0.2500</td> <td>    0.185</td> <td>    1.354</td> <td> 0.209</td> <td>   -0.168</td> <td>    0.668</td>\n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","  <th>Omnibus:</th>       <td> 0.164</td> <th>  Durbin-Watson:     </th> <td>   2.110</td>\n","</tr>\n","<tr>\n","  <th>Prob(Omnibus):</th> <td> 0.921</td> <th>  Jarque-Bera (JB):  </th> <td>   0.261</td>\n","</tr>\n","<tr>\n","  <th>Skew:</th>          <td> 0.208</td> <th>  Prob(JB):          </th> <td>   0.878</td>\n","</tr>\n","<tr>\n","  <th>Kurtosis:</th>      <td> 2.445</td> <th>  Cond. No.          </th> <td>    319.</td>\n","</tr>\n","</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."]},"metadata":{"tags":[]},"execution_count":316}]},{"cell_type":"code","metadata":{"id":"gsTHXPkzPQKK","colab_type":"code","outputId":"6aa63a40-53a6-4af7-d127-e1832fc06bfe","executionInfo":{"status":"ok","timestamp":1556560567069,"user_tz":-120,"elapsed":62,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":535}},"source":["x = df[['x1', 'x2', 'x4']]\n","x = sm.add_constant(x)\n","sm.OLS(y,x).fit().summary()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/Users/Thomas/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:52: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n","  return getattr(obj, method)(*args, **kwds)\n","/Users/Thomas/anaconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1394: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=13\n","  \"anyway, n=%i\" % int(n))\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<class 'statsmodels.iolib.summary.Summary'>\n","\"\"\"\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       0.982\n","Model:                            OLS   Adj. R-squared:                  0.976\n","Method:                 Least Squares   F-statistic:                     166.8\n","Date:                Mon, 29 Apr 2019   Prob (F-statistic):           3.32e-08\n","Time:                        19:56:07   Log-Likelihood:                -26.933\n","No. Observations:                  13   AIC:                             61.87\n","Df Residuals:                       9   BIC:                             64.13\n","Df Model:                           3                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const         71.6483     14.142      5.066      0.001      39.656     103.641\n","x1             1.4519      0.117     12.410      0.000       1.187       1.717\n","x2             0.4161      0.186      2.242      0.052      -0.004       0.836\n","x4            -0.2365      0.173     -1.365      0.205      -0.629       0.155\n","==============================================================================\n","Omnibus:                        0.211   Durbin-Watson:                   2.011\n","Prob(Omnibus):                  0.900   Jarque-Bera (JB):                0.378\n","Skew:                           0.202   Prob(JB):                        0.828\n","Kurtosis:                       2.270   Cond. No.                     1.27e+03\n","==============================================================================\n","\n","Warnings:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","[2] The condition number is large, 1.27e+03. This might indicate that there are\n","strong multicollinearity or other numerical problems.\n","\"\"\""],"text/html":["<table class=\"simpletable\">\n","<caption>OLS Regression Results</caption>\n","<tr>\n","  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.982</td>\n","</tr>\n","<tr>\n","  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.976</td>\n","</tr>\n","<tr>\n","  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   166.8</td>\n","</tr>\n","<tr>\n","  <th>Date:</th>             <td>Mon, 29 Apr 2019</td> <th>  Prob (F-statistic):</th> <td>3.32e-08</td>\n","</tr>\n","<tr>\n","  <th>Time:</th>                 <td>19:56:07</td>     <th>  Log-Likelihood:    </th> <td> -26.933</td>\n","</tr>\n","<tr>\n","  <th>No. Observations:</th>      <td>    13</td>      <th>  AIC:               </th> <td>   61.87</td>\n","</tr>\n","<tr>\n","  <th>Df Residuals:</th>          <td>     9</td>      <th>  BIC:               </th> <td>   64.13</td>\n","</tr>\n","<tr>\n","  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n","</tr>\n","<tr>\n","  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n","</tr>\n","<tr>\n","  <th>const</th> <td>   71.6483</td> <td>   14.142</td> <td>    5.066</td> <td> 0.001</td> <td>   39.656</td> <td>  103.641</td>\n","</tr>\n","<tr>\n","  <th>x1</th>    <td>    1.4519</td> <td>    0.117</td> <td>   12.410</td> <td> 0.000</td> <td>    1.187</td> <td>    1.717</td>\n","</tr>\n","<tr>\n","  <th>x2</th>    <td>    0.4161</td> <td>    0.186</td> <td>    2.242</td> <td> 0.052</td> <td>   -0.004</td> <td>    0.836</td>\n","</tr>\n","<tr>\n","  <th>x4</th>    <td>   -0.2365</td> <td>    0.173</td> <td>   -1.365</td> <td> 0.205</td> <td>   -0.629</td> <td>    0.155</td>\n","</tr>\n","</table>\n","<table class=\"simpletable\">\n","<tr>\n","  <th>Omnibus:</th>       <td> 0.211</td> <th>  Durbin-Watson:     </th> <td>   2.011</td>\n","</tr>\n","<tr>\n","  <th>Prob(Omnibus):</th> <td> 0.900</td> <th>  Jarque-Bera (JB):  </th> <td>   0.378</td>\n","</tr>\n","<tr>\n","  <th>Skew:</th>          <td> 0.202</td> <th>  Prob(JB):          </th> <td>   0.828</td>\n","</tr>\n","<tr>\n","  <th>Kurtosis:</th>      <td> 2.270</td> <th>  Cond. No.          </th> <td>1.27e+03</td>\n","</tr>\n","</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.27e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."]},"metadata":{"tags":[]},"execution_count":317}]},{"cell_type":"markdown","metadata":{"id":"wXN_sugcweql","colab_type":"text"},"source":["## Boston dataset"]},{"cell_type":"code","metadata":{"id":"e3iqiA7WwgPW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"outputId":"d5d70bf2-aeac-4dc2-fd5b-50a87dae7d93","executionInfo":{"status":"ok","timestamp":1557240637861,"user_tz":-120,"elapsed":981,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}}},"source":["headers = ['CRIM','ZN','INDUS','NOX','RM','AGE','DIS','TAX','PTRATIO','MEDV']\n","range_name = 'boston_train.csv!A2:J402'\n","\n","df = service['sheet'].spreadsheets().values().get(\n","    spreadsheetId='1IwxJCod9CKHim14ltdHlJwnYZktVcaCAS5d0pjvgaTM',\n","    range=range_name).execute()\n","df = pd.DataFrame(df.get('values', []), columns=headers)\n","df = df.apply(pd.to_numeric, errors='ignore')\n","df.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       CRIM    ZN  INDUS    NOX     RM   AGE     DIS  TAX  PTRATIO  MEDV\n","0   2.30040   0.0  19.58  0.605  6.319  96.1  2.1000  403     14.7  23.8\n","1  13.35980   0.0  18.10  0.693  5.887  94.7  1.7821  666     20.2  12.7\n","2   0.12744   0.0   6.91  0.448  6.770   2.9  5.7209  233     17.9  26.6\n","3   0.15876   0.0  10.81  0.413  5.961  17.5  5.2873  305     19.2  21.7\n","4   0.03768  80.0   1.52  0.404  7.274  38.3  7.3090  329     12.6  34.6"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>CRIM</th>\n","      <th>ZN</th>\n","      <th>INDUS</th>\n","      <th>NOX</th>\n","      <th>RM</th>\n","      <th>AGE</th>\n","      <th>DIS</th>\n","      <th>TAX</th>\n","      <th>PTRATIO</th>\n","      <th>MEDV</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2.30040</td>\n","      <td>0.0</td>\n","      <td>19.58</td>\n","      <td>0.605</td>\n","      <td>6.319</td>\n","      <td>96.1</td>\n","      <td>2.1000</td>\n","      <td>403</td>\n","      <td>14.7</td>\n","      <td>23.8</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>13.35980</td>\n","      <td>0.0</td>\n","      <td>18.10</td>\n","      <td>0.693</td>\n","      <td>5.887</td>\n","      <td>94.7</td>\n","      <td>1.7821</td>\n","      <td>666</td>\n","      <td>20.2</td>\n","      <td>12.7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.12744</td>\n","      <td>0.0</td>\n","      <td>6.91</td>\n","      <td>0.448</td>\n","      <td>6.770</td>\n","      <td>2.9</td>\n","      <td>5.7209</td>\n","      <td>233</td>\n","      <td>17.9</td>\n","      <td>26.6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.15876</td>\n","      <td>0.0</td>\n","      <td>10.81</td>\n","      <td>0.413</td>\n","      <td>5.961</td>\n","      <td>17.5</td>\n","      <td>5.2873</td>\n","      <td>305</td>\n","      <td>19.2</td>\n","      <td>21.7</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.03768</td>\n","      <td>80.0</td>\n","      <td>1.52</td>\n","      <td>0.404</td>\n","      <td>7.274</td>\n","      <td>38.3</td>\n","      <td>7.3090</td>\n","      <td>329</td>\n","      <td>12.6</td>\n","      <td>34.6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"xkWC3Ef5xnGN","colab_type":"code","colab":{}},"source":["stepwise_regression(df, X = ['CRIM','ZN','INDUS','NOX','RM','AGE','DIS',\n","                             'TAX','PTRATIO'],\n","                    y = 'MEDV')"],"execution_count":0,"outputs":[]}]}