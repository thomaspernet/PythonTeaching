{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"11_DecisionTree.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wFVsxB_xJyyn","colab_type":"text"},"source":["# Decision tree\n","\n","From [Source1](https://www.saedsayad.com/decision_tree.htm)\n","\n","Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed\n","\n","The core algorithm for building decision trees called ID3 by J. R. Quinlan which employs a top-down, greedy search through the space of possible branches with no backtracking.\n","\n","ID3 uses Entropy and Information Gain to construct a decision tree.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UwQQTPHnKBLY","colab_type":"text"},"source":["## Information gain\n","\n","A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous). ID3 algorithm uses entropy to calculate the homogeneity of a sample. If the sample is completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one.\n","\n","Information gain (IG) measures how much \"information\" a feature gives us about the class.\n","\n","- Features that perfectly partition should give maximal information.\n","- Unrelated features should give no information.\n","\n","Suppose we want to split on the first variable (ùë•1):\n","\n","If we split at $x_1 < 3.5$ , we get an optimal split\n","I we split at $ x_1 < 4$, wz make a mistake\n","\n","- Idea: A better split should make the samples ‚Äúpure‚Äù (homogeneous).\n","\n","Impurity/Entropy (informal)\n","\n","- Measures the level of impurity in a group of examples\n","\n","![Example impurtiy](https://dynalist.io/u/Wd8A6pPKQ-7L1FuYDVEsnR0J)\n","\n","### Measures for Selecting the Best Split\n","\n","- Impurity measures include:\n","\n","$$\\text { Entropy }=-\\sum_{i=1}^{K} p_{k} \\log _{2} p_{k}$$\n","$$\\text { Gini }=1-\\sum_{i=1}^{K} p_{k}^{2}$$\n","$$\\text { Classification error }=1-\\max _{i} p_{\\text { en }}$$\n","\n","![Entropy](https://dynalist.io/u/COM8fPj_0f1SY6j0LTbhLmNb)\n","\n","- 16/30 are green circles; \n","- 14/30 are pink crosses\n","  - log2(16/30) = -.9; log2(14/30) = -1.1\n","- Entropy = -(16/30)(-.9) ‚Äì(14/30)(-1.1) = .99 \n","\n","- Entropy comes from information theory. The higher the entropy the more the information content.\n","  - Information gain tells us how important a given attribute of the feature vectors is\n","  - We will use it to decide the ordering of attributesin the nodes of a decision tree.\n","- the change in entropy, or Information Gain, is defined as: \n","\n","$$\\Delta H=H-\\frac{m_{L}}{m} H_{L}-\\frac{m_{R}}{m} H_{R}$$\n","\n","where $ùëö$ is the total number of instances, with $ùëö_ùëò$ instances belonging to class $ùëò$, where $ùêæ = 1, ‚Ä¶ , ùëò.$\n","\n","$$\\text { Information Gain }=\\quad \\text { entropy (parent) }-[\\text { average entropy (children) }]$$\n","\n","**Example**\n","\n","The column `Play` is the parent column and the remaining columns are the childs\n","\n","- 5 No\n","- 9 Yes\n","\n","|  Outlook | Temperature | Humidity | Windy | Play |\n","| -------- | ----------- | -------- | ----- | ---- |\n","| Sunny    | Hot         | High     | False | No   |\n","| Sunny    | Hot         | High     | True  | No   |\n","| Overcast | Hot         | High     | False | Yes  |\n","| Rainy    | Mild        | High     | False | Yes  |\n","| Rainy    | Cool        | Normal   | False | Yes  |\n","| Rainy    | Cool        | Normal   | True  | No   |\n","| Overcast | Cool        | Normal   | True  | Yes  |\n","| Sunny    | Mild        | High     | False | No   |\n","| Sunny    | Cool        | Normal   | False | Yes  |\n","| Rainy    | Mild        | Normal   | False | Yes  |\n","| Sunny    | Mild        | Normal   | True  | Yes  |\n","| Overcast | Mild        | High     | True  | Yes  |\n","| Overcast | Hot         | Normal   | False | Yes  |\n","| Rainy    | Mild        | High     | True  | No   |\n","|          |             |          |       |      |\n","\n","\n","#### Parent \n","\n","$$\\begin{aligned} & \\mathrm{H}(Y)=-\\sum_{i=1}^{K} p_{k} \\log _{2} p_{k} \\\\=&-\\frac{5}{14} \\log _{2} \\frac{5}{14}-\\frac{9}{14} \\log _{2} \\frac{9}{14} \\\\=& 0.94 \\end{aligned}$$\n","\n","#### Childs\n","\n","**Humidity**\n","\n","There are 7 high and 7 normal. We use the parent computation $H(Y)$ and substract to the information gain from `Humidity`\n","\n","$$\\begin{array}{l}{\\text {InfoGain}(\\text { Humidity })=} \\\\ {H(Y)-\\frac{m_{L}}{m} H_{L}-\\frac{m_{R}}{m} H_{R}} \\\\ {=0.94-\\frac{7}{14} H_{L}-\\frac{7}{14} H_{R}}\\end{array}$$\n","\n","Now we need to compute $H_L$ using the entropy formula. Among the 7 `Normal` label, 6 belongs to `Yes` and 1 to  `No`\n","\n","$$\\begin{array}{c}{\\text {InfoGain}(\\text { Humidity })=} \\\\ {\\quad H(Y)-\\frac{m_{L}}{m} H_{L}-\\frac{m_{R}}{m} H_{R}} \\\\ {0.94-\\frac{7}{14} H_{L}-\\frac{7}{14} H_{R}} \\\\ {H_{L}=-\\frac{6}{7} \\log _{2} \\frac{6}{7}-\\frac{1}{7} \\log _{2} \\frac{1}{7}}\\end{array}$$\n","\n","We repeat the same exercice for $H_R$\n","\n","$$H_{R}=-\\frac{3}{7} \\log _{2} \\frac{3}{7}-\\frac{4}{7} \\log _{2} \\frac{4}{7}$$\n","\n","All together, we get:\n","\n","$$\\begin{array}{l}{\\text {InfoGain}(\\text { Humidity })=} \\\\ {H(Y)-\\frac{m_{L}}{m} H_{L}-\\frac{m_{R}}{m} H_{R}} \\\\ {0.94-\\frac{7}{14} 0.592-\\frac{7}{14} 0.985} \\\\ {=0.94-0.296-0.4925} \\\\ {=0.1515}\\end{array}$$\n","\n","We compute the information gain for each feature\n","\n","-  Information gain for each feature:\n","  -  Outlook = 0.247\n","  - Temperature = 0.029\n","  -  Humidity = 0.152\n","  - Windy = 0.048\n","  \n"," - Initial split is on outlook, because it is the feature with the highest information gain. \n"," -  In the next step, we look for the second best split:\n","  - Temperature: .571\n","  - Windy: .020\n","  - Humidity: .971\n","  \n","  The seconds winner are Temperature and Humidity\n"," \n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"xhWU4nzJ1YGv","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gQW9t73FrAy5","colab_type":"code","colab":{}},"source":["def entropy(p):\n","  \"\"\"\n","  Define entropy. \n","  \"\"\"\n","  \n","  d_entropy = p * np.log2(p)\n","  \n","  return d_entropy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DMPVmZLRJupX","colab_type":"code","outputId":"fdd526e6-ce8d-46d8-cc48-9fed6b2ac904","executionInfo":{"status":"ok","timestamp":1560352732367,"user_tz":-120,"elapsed":1808,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["entropy(0.3)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-0.5210896782498619"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"iWyETnkB1qRN","colab_type":"text"},"source":["### Define information gain\n","\n","[Source](https://www.python-course.eu/Decision_Trees.php)\n","\n","Entropy defines the impurity of the feature. \n","\n","our task is to find the best feature in terms of information gain (Remember that we want to find the feature which splits the data most accurate along the target feature values) which we should use to first split our data on (which serves as root node)\n","\n","how can we check which of the descriptive features most accurately splits the dataset, that is, remains the dataset with the lowest impurity ‚âà entropy or in other words best classifies the target features by its own\n","\n","we use each descriptive feature and split the dataset along the values of these descriptive feature and then calculate the entropy of the dataset once we have split the data along the feature values. This gives us the remaining entropy after we have split the dataset along the feature values.\n","\n","$$\n","\\text {InfoGain}\\left(\\text {feature}_{d}\\right)=\\text {Entropy}(D)-\\text {Entropy (featured} )\n","$$\n","\n","$$InforGain(feature_{d},D) = Entropy(D)-\\sum_{t \\ \\in \\ feature}(\\frac{|feature_{d} = t|}{|D|}*H(feature_{d} = t))$$\n","\n","$$Entropy(D)-\\sum_{t \\ \\in \\ feature}(\\frac{|feature_{d} = t|}{|D|}*(-\\sum_{k \\ \\in \\ target}(P(target=k,feature_{d} = t)*log_{2}(P(target = k,feature_{d} = t))))$$\n","\n","For each column, we end up with $k$ * $n$ \"entropy\" computation, where $k$  is the class of the column to predict and $n$ the number of class in the feature. \n","\n","If the column to predict has 2 classes and the feature column has 2 class as well, we end up adding 4 $H(feature)$ together on top of the $H(Y)$\n"]},{"cell_type":"code","metadata":{"id":"eVY3nxnz3GrG","colab_type":"code","colab":{}},"source":["import pandas as pd\n","\n","Outlook = ['Sunny', 'Sunny', 'Overcast', 'Rainy', 'Rainy','Rainy', 'Overcast',\n","           'Sunny', 'Sunny', 'Rainy', 'Sunny', 'Overcast', 'Overcast', 'Rainy'\n","          ]\n","Temperature = ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild',\n","               'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild']\n","Humidity = ['High', 'High', 'High' ,'High', 'Normal', 'Normal', 'Normal',\n","            'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal',\n","            'High']\n","Windy = [False, True, False, False, False, True, True, False, False, False,\n","        True, True, False, True]\n","Play = ['No', 'No','Yes', 'Yes', 'Yes', 'No', 'Yes', 'No',\n","        'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\n","\n","df_test = pd.DataFrame( {'Outlook': Outlook, 'Temperature': Temperature,\n","               'Humidity': Humidity, 'Windy': Windy, 'Play':Play})\n","df_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3F3Of3zQ6lvD","colab_type":"code","outputId":"d8604e6d-5677-4bc6-d176-2fc748728342","executionInfo":{"status":"ok","timestamp":1560352752906,"user_tz":-120,"elapsed":566,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":335}},"source":["import pandas as pd\n","#df_test.iloc[:, :-1].apply(entropy)\n","data = pd.DataFrame({\"toothed\":[\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\"],\n","                     \"breathes\":[\"True\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\"],\n","                     \"legs\":[\"Between\",\"Between\",\"False\",\"Between\",\"True\",\"True\",\"Between\",\"False\",\"True\",\"True\"],\n","                     \"species\":[\"Mammal\",\"Mammal\",\"Reptile\",\"Mammal\",\"Mammal\",\"Mammal\",\"Reptile\",\"Reptile\",\"Mammal\",\"Reptile\"]}, \n","                    columns=[\"toothed\",\"breathes\",\"legs\",\"species\"])\n","features = data[[\"toothed\",\"breathes\",\"legs\"]]\n","target = data[\"species\"]\n","data"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>toothed</th>\n","      <th>breathes</th>\n","      <th>legs</th>\n","      <th>species</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>Between</td>\n","      <td>Mammal</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>Between</td>\n","      <td>Mammal</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>Reptile</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>Between</td>\n","      <td>Mammal</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>Mammal</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>Mammal</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>Between</td>\n","      <td>Reptile</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>False</td>\n","      <td>Reptile</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>Mammal</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>False</td>\n","      <td>True</td>\n","      <td>True</td>\n","      <td>Reptile</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  toothed breathes     legs  species\n","0    True     True  Between   Mammal\n","1    True     True  Between   Mammal\n","2    True     True    False  Reptile\n","3   False     True  Between   Mammal\n","4    True     True     True   Mammal\n","5    True     True     True   Mammal\n","6    True    False  Between  Reptile\n","7    True     True    False  Reptile\n","8    True     True     True   Mammal\n","9   False     True     True  Reptile"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"mYPaZh9I25Jm","colab_type":"code","colab":{}},"source":["def informationGains(features, y):\n","  \"\"\"\n","  Define information gains.\n","  \n","  Pass a pandas Dataframe and compute information gain\n","  column wise\n","  \"\"\"\n","  \n","  ### List features\n","  \n","  list_f = []\n","  \n","  ### List Gain information\n","  \n","  list_gain = []\n","  \n","  ### Name target column \n","  target_name = y.name\n","  \n","  ### Compute Entropy Y\n","  \n","  feature_class_y = y.value_counts(normalize=True)\n","  entropy_Y = np.abs(feature_class_y.apply(entropy).sum())\n","  \n","  ### Concat target and features\n","  df_concat = pd.concat([features, y], axis = 1)\n"," \n","  \n","  for col in features.columns:\n","  ### Compute count by feature class\n","    feature_class = df_concat.groupby([col, target_name]).agg({col: 'count'})\n","  \n","  ### Compute the share within each class of the label feature\n","    within_feature_class = feature_class.groupby(level=0).apply(\n","      lambda x: x / float(x.sum()))\n","  \n","  ### Compute the share between each class of the label feature\n","    between_feature_class = feature_class.groupby(level=0).agg({col: 'sum'})\n","    between_feature_class = between_feature_class.apply(\n","      lambda x: x / float(x.sum()))\n","  \n","    df_unstack = within_feature_class.unstack()\n","    df_unstack = df_unstack.fillna(0)\n","    within_entropy = df_unstack.apply(entropy, axis = 1)\n","    within_entropy = within_entropy.fillna(0)\n","  ### compute sum within share class\n","    sum_entropy_class = within_entropy.apply(sum, axis= 1)\n","  \n","  ### Compute dot product share between with sum share within\n","    gain_information = entropy_Y - \\\n","    np.abs(np.dot(sum_entropy_class, between_feature_class))\n","    \n","    list_f.append([col])\n","    list_gain.append(gain_information)\n","    \n","  max_gain =  np.max(list_gain)\n","  max_feature = list_f[list_gain.index(max_gain)]\n","\n","  ### Prepare dataset for next split\n","  \n","  label_winner = df_concat[max_feature[0]].unique()\n","  \n","  #### Get stop if target is purely separeted\n","  \n","  ##### Need to store the new dataframe for the next leaf\n","  l_new_df_features = []\n","  l_new_df_label = []\n","  l_new_class = []\n","   \n","  for label_ in label_winner:\n","    \n","    is_winner = df_concat[df_concat[max_feature[0]] ==\n","                    label_].iloc[:, -1].unique()\n","    if len(is_winner) == 1:\n","  \n","  ### Done splitting\n","      winner_split  = label_\n","      target_label = is_winner[0]\n","    else:\n","  ### Get next dataframe\n","      new_df_features = df_concat[df_concat[max_feature[0]] ==\n","                    label_].iloc[:,:-1].drop(columns = max_feature[0])\n","      new_df_label = df_concat[df_concat[max_feature[0]] ==\n","                    label_].iloc[:,-1]\n","      \n","      l_new_df_features.append(new_df_features)\n","      l_new_df_label.append(new_df_label)\n","      l_new_class.append(label_)\n","      \n","      \n","  try:\n","    winner_split\n","  except:\n","    winner_split = None\n","    target_label = None\n","  \n","  #### Need to review results: Should match the parent node\n","  df_gain = {\n","        'features' : list_f,\n","        'gain_information' : list_gain,\n","        'winner': {\n","            'feature':max_feature,\n","            'gain_information': max_gain,\n","            'label': winner_split,\n","            'label_target':target_label\n","        },\n","      'next_split': {\n","          'features': {\n","              'class':l_new_class,\n","              'df': l_new_df_features,\n","          'target': l_new_df_label\n","          }\n","      }\n","        \n","    }\n","  \n","  return df_gain"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A24v3LXVm_uV","colab_type":"code","outputId":"a5268a18-c1e7-4600-b27f-bd9af0eb9d11","executionInfo":{"status":"ok","timestamp":1560356784593,"user_tz":-120,"elapsed":860,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":66}},"source":["test = informationGains(features = features, y= target)\n","test['winner']['label']"],"execution_count":104,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log2\n","  \n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["'False'"]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"code","metadata":{"id":"3a_K3pyZeyXP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"03b2d4ff-8712-4757-b1c7-c0ca03f5767a","executionInfo":{"status":"ok","timestamp":1560356341750,"user_tz":-120,"elapsed":672,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}}},"source":["for key, value in enumerate(test['next_split']['features']['class']):\n","  print(value)"],"execution_count":99,"outputs":[{"output_type":"stream","text":["Between\n","True\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TOtApzcy96RQ","colab_type":"text"},"source":["Hence the splitting the dataset along the feature legs results in the largest information gain and we should use this feature for our root node."]},{"cell_type":"markdown","metadata":{"id":"FExcMJ5YDAsV","colab_type":"text"},"source":["We see that for legs == False, the target feature values of the remaining dataset are all Reptile and hence we set this as leaf node because we have a pure dataset (Further splitting the dataset on any of the remaining two features would not lead to a different or more accurate result since whatever we do after this point, the prediction will remain Reptile). \n","\n","Additionally, you see that the feature legs is no longer included in the remaining datasets. Because we already has used this (categorical) feature to split the dataset on it must not be further used.\n"]},{"cell_type":"code","metadata":{"id":"y0QkYhnyJyG6","colab_type":"code","colab":{}},"source":["def build_tree(features, target):\n","  \"\"\"\n","  Build decision tree.\n","  \n","  Loop until no more column\n","  \"\"\"\n","\n","  l_nodes = []\n","  l_features = []\n","  l_stops = []\n","  l_gaininfo = []\n","  l_target_winner = []\n"," \n","  \n","  #### Set first pass\n","  features_ = [features]\n","  target_ = [target]\n"," \n","  #### there are two loops: \n","  ##### First, we loop over the dataframes\n","  ##### Second, we loop over the columns\n","  ##### Each loop will give us one branch and leaves of the tree\n","  i = 0\n","  while len(features_) > 0:\n","    for n, df in enumerate(features_):\n","      \n","      #n_col = len(df.columns) \n","    #for i in range(0, n_col):\n","      print(i, n)\n","      df_ = informationGains(features = df, y = target_[n])\n","      \n","      print(df_['winner']['feature'], df_['winner']['label'])\n","      for key, value in enumerate(df_['next_split']['features']['class']):\n","        print(value)\n","    #print(df_['next_split']['features']['class'][n])\n","      \n","  \n","      #### Store the results \n","      #l_nodes.append(1)\n","      #l_features.append(df_['winner']['feature'])\n","      #l_stops.append(df_['winner']['label'])\n","      #l_gaininfo.append(df_['winner']['gain_information'])\n","      #l_target_winner.append(df_['winner']['label_target'])\n","  \n","    features_ = df_['next_split']['features']['df']\n","    target_ = df_['next_split']['features']['target']\n","    i=+1\n","  dic_tree = {\n","    'node': l_nodes,\n","    'feature': l_features,\n","    'stop': l_stops,\n","    'gain_information': l_gaininfo,\n","    'label_target': l_target_winner,\n","    \n","    }\n","  return dic_tree"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mnuk8cI3KjEh","colab_type":"code","outputId":"381b5011-18ea-457e-a71b-fc387de440ca","executionInfo":{"status":"ok","timestamp":1560356841802,"user_tz":-120,"elapsed":609,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":180}},"source":["features_ = features\n","target_ = target\n","test = build_tree(features = features_, target = target_)"],"execution_count":110,"outputs":[{"output_type":"stream","text":["0 0\n","['legs'] False\n","Between\n","True\n","1 0\n","['breathes'] False\n","1 1\n","['toothed'] False\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log2\n","  \n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"iwZZDYfBkdmy","colab_type":"code","colab":{}},"source":["list_ = ['False', 'Between', 'True']\n","\n","dic_test ={}\n","\n","dic_test.update({'legs' : {'False'}})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKW99lTShj_M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"9b36ef8a-36b3-4626-fb4b-b3ae7d90ae1e","executionInfo":{"status":"ok","timestamp":1560357250826,"user_tz":-120,"elapsed":658,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}}},"source":["dic_test"],"execution_count":123,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'legs': {'Between'}}"]},"metadata":{"tags":[]},"execution_count":123}]},{"cell_type":"code","metadata":{"id":"P_A08rZwj-tA","colab_type":"code","colab":{}},"source":["dic_test.update({'legs' : {'Between'}})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ttDowTS6YtBw","colab_type":"text"},"source":["Mind the last split (node) where the dataset got split on the breathes feature. Here the breathes feature solely contains data where breaths == True. Hence for breathes == False there are no instances in the dataset and therewith there is no sub-Dataset which can be built. In that case we return the most frequently occurring target feature value in the original dataset which is Mammal. This is an example how our tree model generalizes behind the training data.\n","\n","If we consider the other branch, that is breathes == True we know, that after splitting the Dataset on the values of a specific feature (breathes {True,False}) in our case, the feature must be removed. Well, that leads to a dataset where no more features are available to further split the dataset on. Hence we stop growing the tree and return the mode value of the direct parent node which is \"Mammal\""]},{"cell_type":"markdown","metadata":{"id":"qXXPOCbdY2EG","colab_type":"text"},"source":["# ID3 Algorithm\n","\n","That leads us to the introduction of the ID3 algorithm which is a popular algorithm to grow decision trees, published by Ross Quinlan in 1986. Besides the ID3 algorithm there are also other popular algorithms like the C4.5, the C5.0 and the CART algorithm which we will not further consider here. Before we introduce the ID3 algorithm lets quickly come back to the stopping criteria of the above grown tree.\n","\n","There are mainly three useful cases in which we stop the tree from growing assuming we do not stop it beforehand by defining for instance a maximum tree depth or a minimum information gain value. \n","\n","We stop the tree from growing when:\n","\n","1. All rows in the target feature have the same value \n","2. The dataset can be no longer split since there are no more features left\n","3. The dataset can no longer be split since there are no more rows left / There is no data left\n","\n","By definition, we say that if the growing gets stopped because of stopping criteria two, the leaf node should predict the most frequently occurring target feature value of the superior (parent) node. If the growing gets stopped because of the third stopping criteria, we assign the leaf node the mode target feature value of the original dataset."]},{"cell_type":"markdown","metadata":{"id":"GDLgWImCaiyr","colab_type":"text"},"source":["## Example\n","\n","Since we now know the principal steps of the ID3 algorithm, we will start create our own decision tree classification model from scratch in Python.\n","\n","Therefore we will use the whole UCI Zoo Data Set.\n","This dataset consists of 101 rows and 17 categorically valued attributes defining whether an animal has a specific property or not (e.g.hairs, feathers,..). The first attribute represents the name of the animal and will be removed. The target feature consist of 7 integer values [1 to 7] which represents [1:Mammal, 2:Bird, 3:Reptile, 4:Fish, 5:Amphibian, 6:Bug, 7:Invertebrate]\n","\n","That said, there are four important steps:\n","\n","1. The calculation of the Information Gain\n","2. The recursive call of the TreeModel\n","3. The building of the actual tree structure\n","4. The species prediction of a new unseen animal-instance\n","\n","Here the most critical aspects are the recursive call of the TreeModel, the creation of the tree itself (building the tree structure) as well as the prediction of a unseen query instance (the process of wandering down the tree to predict the class of a unseen query instance)."]},{"cell_type":"code","metadata":{"id":"AwKsgbAFY4DO","colab_type":"code","outputId":"c5b09b8f-d8e5-4b30-d3b7-e015de89d788","executionInfo":{"status":"ok","timestamp":1560321817739,"user_tz":-120,"elapsed":765,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":211}},"source":["dataset=pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/zoo/zoo.data',\n","           names=['animal_name','hair','feathers','eggs','milk',\n","                  'airbone','aquatic','predator','toothed','backbone',\n","                  'breathes','venomous','fins','legs','tail','domestic',\n","                  'catsize','class'])\n","dataset=dataset.drop('animal_name',axis=1)\n","dataset.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hair</th>\n","      <th>feathers</th>\n","      <th>eggs</th>\n","      <th>milk</th>\n","      <th>airbone</th>\n","      <th>aquatic</th>\n","      <th>predator</th>\n","      <th>toothed</th>\n","      <th>backbone</th>\n","      <th>breathes</th>\n","      <th>venomous</th>\n","      <th>fins</th>\n","      <th>legs</th>\n","      <th>tail</th>\n","      <th>domestic</th>\n","      <th>catsize</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   hair  feathers  eggs  milk  airbone  ...  legs  tail  domestic  catsize  class\n","0     1         0     0     1        0  ...     4     0         0        1      1\n","1     1         0     0     1        0  ...     4     1         0        1      1\n","2     0         0     1     0        0  ...     0     1         0        0      4\n","3     1         0     0     1        0  ...     4     0         0        1      1\n","4     1         0     0     1        0  ...     4     1         0        1      1\n","\n","[5 rows x 17 columns]"]},"metadata":{"tags":[]},"execution_count":148}]},{"cell_type":"markdown","metadata":{"id":"qfskygXNbjOn","colab_type":"text"},"source":["We run the algorithm until no more rows in the features. \n","\n","First of all, let's try the algorithm without train/test split"]},{"cell_type":"code","metadata":{"id":"XX72m3Plb91K","colab_type":"code","colab":{}},"source":["X = dataset.iloc[:, :-1]\n","Y = dataset.iloc[:, -1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qZj1mFp7c5ro","colab_type":"code","outputId":"49baae66-41b1-4ada-ef47-7eb8ad6b800c","executionInfo":{"status":"ok","timestamp":1560322455924,"user_tz":-120,"elapsed":1049,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":1028}},"source":["informationGains(features = X, y = Y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log2\n","  \n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'features': [['hair'],\n","  ['feathers'],\n","  ['eggs'],\n","  ['milk'],\n","  ['airbone'],\n","  ['aquatic'],\n","  ['predator'],\n","  ['toothed'],\n","  ['backbone'],\n","  ['breathes'],\n","  ['venomous'],\n","  ['fins'],\n","  ['legs'],\n","  ['tail'],\n","  ['domestic'],\n","  ['catsize']],\n"," 'gain_information': [array([0.79067457]),\n","  array([0.71794998]),\n","  array([0.83013845]),\n","  array([0.97431972]),\n","  array([0.46970261]),\n","  array([0.38948748]),\n","  array([0.09344704]),\n","  array([0.86569415]),\n","  array([0.67616274]),\n","  array([0.61449403]),\n","  array([0.13308963]),\n","  array([0.46661357]),\n","  array([1.3630469]),\n","  array([0.50046045]),\n","  array([0.05066878]),\n","  array([0.30849034])],\n"," 'next_split': {'features':     hair  feathers  eggs  milk  ...  fins  tail  domestic  catsize\n","  15     0         0     1     0  ...     0     0         0        0\n","  24     0         0     1     0  ...     0     0         0        0\n","  30     0         0     1     0  ...     0     0         0        0\n","  39     1         0     1     0  ...     0     0         1        0\n","  40     1         0     1     0  ...     0     0         0        0\n","  42     0         0     1     0  ...     0     0         0        0\n","  46     0         0     1     0  ...     0     0         0        0\n","  51     1         0     1     0  ...     0     0         0        0\n","  88     0         0     1     0  ...     0     0         0        0\n","  97     1         0     1     0  ...     0     0         0        0\n","  \n","  [10 rows x 15 columns], 'target': 15    7\n","  24    6\n","  30    6\n","  39    6\n","  40    6\n","  42    6\n","  46    7\n","  51    6\n","  88    6\n","  97    6\n","  Name: class, dtype: int64},\n"," 'winner': {'feature': ['legs'],\n","  'gain_information': 1.3630469031539394,\n","  'label': 5,\n","  'label_target': 7}}"]},"metadata":{"tags":[]},"execution_count":167}]},{"cell_type":"code","metadata":{"id":"Gdxc-uG_b2HI","colab_type":"code","outputId":"43c254bd-069f-466e-abe4-1dc30c80d0bd","executionInfo":{"status":"error","timestamp":1560322181849,"user_tz":-120,"elapsed":1186,"user":{"displayName":"Thomas Pernet","photoUrl":"https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg","userId":"12184844431429654860"}},"colab":{"base_uri":"https://localhost:8080/","height":462}},"source":["build_tree(features = X, target = Y)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log2\n","  \n"],"name":"stderr"},{"output_type":"error","ename":"UnboundLocalError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-164-82e805f8e7ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbuild_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-142-cf6d9a9f910e>\u001b[0m in \u001b[0;36mbuild_tree\u001b[0;34m(features, target)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mdf_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minformationGains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#### Store the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-161-dc491de72198>\u001b[0m in \u001b[0;36minformationGains\u001b[0;34m(features, y)\u001b[0m\n\u001b[1;32m     92\u001b[0m         },\n\u001b[1;32m     93\u001b[0m       'next_split': {\n\u001b[0;32m---> 94\u001b[0;31m           \u001b[0;34m'features'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnew_df_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m           \u001b[0;34m'target'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_df_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m       }\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'new_df_features' referenced before assignment"]}]}]}