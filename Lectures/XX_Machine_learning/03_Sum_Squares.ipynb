{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "V5lIB3vzPkL5"
   },
   "source": [
    "# Sums of squares\n",
    "\n",
    "Source: [PennState Stat 501](https://newonlinecourses.science.psu.edu/stat501/node/263/)\n",
    "\n",
    "\"Is there a (linear) relationship between skin cancer mortality and latitude?\"\n",
    "\n",
    "Review the following scatter plot and estimated regression line. What does the plot suggest is the answer to the research question? The linear relationship looks fairly strong. The estimated slope is negative, not equal to 0\n",
    "\n",
    "## Least Squares Estimates\n",
    "\n",
    "In a simple OLS regression, the computation of $\\alpha_0$ and $\\beta_0$ is straightforward. We don't mean to show the derivation in this tutorial. We will only write the formula.\n",
    "\n",
    "We want to estimate: $y = \\beta_0+\\beta_1x+\\varepsilon$\n",
    "\n",
    "The goal of the OLS regression is to minimize the following equation:\n",
    "\n",
    "$\\sum (y_i-\\hat{y}_i)^2= \\sum e_i^2$\n",
    "\n",
    "where $y_i$ is the actual values and $\\hat{y}_i$ is the predicted values.\n",
    "\n",
    "The solution for $\\beta$ is $\\beta_0 = \\bar{y}-\\beta_1\\bar{x}$\n",
    "\n",
    "Note that $\\bar{x}$ means the average value of $x$\n",
    "\n",
    "The solution for $\\beta$ is $\\beta = \\frac{Cov(x,y)}{Var(x)}$\n",
    "\n",
    "## Multiple Linear regression\n",
    "\n",
    "More practical applications of regression analysis employ models that are more complex than the simple straight-line model. The probabilistic model that includes more than one independent variable is called multiple regression models. The general form of this model is:\n",
    "\n",
    "$y = \\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_kx_k+\\varepsilon$\n",
    "\n",
    "In matrix notation, you can rewrite the model:\n",
    "\n",
    "$Y = \\beta X+\\varepsilon$\n",
    "The dependent variable $y$ is now a function of $k$ independent variables. The value of the coefficient $\\beta_i$ determines the contribution of the independent variable $x_i$ and $\\beta_0$.\n",
    "\n",
    "We briefly introduce the assumption you make about the random error $\\varepsilon$ of the OLS:\n",
    "\n",
    "- Mean equal to 0\n",
    "- Variance equal to $\\sigma^2$\n",
    "- Normal distribution\n",
    "    - Random errors are independent (in a probabilistic sense)\n",
    "    \n",
    "You need to solve for $\\beta$, the vector of regression coefficients that minimise the sum of the squared errors between the predicted and actual $Y$ values.\n",
    "\n",
    "The closed-form solution is:\n",
    "\n",
    "$\\beta = (X^TX)^{-1}X^TY$\n",
    "\n",
    "with:\n",
    "\n",
    "$T$ indicates the transpose of the matrix X\n",
    "$(X^TX)^{-1}$ indicates the invertible matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "YEDKjsw7P1SY"
   },
   "source": [
    "## Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3719,
     "status": "ok",
     "timestamp": 1556359388172,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "p7ksZMTnPfs4",
    "outputId": "ca698179-f524-4a68-aadc-39394682bbbc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Mort</th>\n",
       "      <th>Ocean</th>\n",
       "      <th>Long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>33.0</td>\n",
       "      <td>219</td>\n",
       "      <td>1</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>34.5</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>35.0</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>92.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>California</td>\n",
       "      <td>37.5</td>\n",
       "      <td>182</td>\n",
       "      <td>1</td>\n",
       "      <td>119.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>39.0</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>105.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        State   Lat  Mort  Ocean   Long\n",
       "0     Alabama  33.0   219      1   87.0\n",
       "1     Arizona  34.5   160      0  112.0\n",
       "2    Arkansas  35.0   170      0   92.5\n",
       "3  California  37.5   182      1  119.5\n",
       "4    Colorado  39.0   149      0  105.5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "df = pd.read_excel('skincancer.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "7pkfv0G1P6kH"
   },
   "source": [
    "## Fitted values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1556359607076,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "O8Q7LHroP-2x",
    "outputId": "b4971847-1515-46b8-8c81-ac1b7a183ac1"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = df['Mort']\n",
    "x = df['Lat']\n",
    "\n",
    "#data_x = [30, 35]\n",
    "#data_y = [209.86, 141]\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "print(\"slope: %f    intercept: %f\" % (slope, intercept))\n",
    "print(\"p-values: %f\" % (p_value))\n",
    "print(\"r-squared: %f\" % r_value**2)\n",
    "  \n",
    "#### Plot with label\n",
    "labels = [r'$\\mu_{Y} = E(Y)=\\beta_{0}+\\beta_{1} X$',\n",
    "          r'$Y_{i}= \\beta_{0}+\\beta_{1} X+\\varepsilon_{i}$']\n",
    "\n",
    "plt.plot(x, y, 'o', label='original data')\n",
    "plt.plot(x, intercept + slope * x, 'r', label='fitted line')\n",
    "\n",
    "for label, x_1, y_2 in zip(labels, data_x, data_y):\n",
    "      plt.annotate(label,\n",
    "            xy=(x_1, y_2), xycoords='data',\n",
    "            xytext=(-45, -50),\n",
    "            textcoords='offset points',\n",
    "            arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Mortality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "0n3StuZMQL6e"
   },
   "source": [
    "## Research Question\n",
    "\n",
    "We can answer the research question using the P-value of the t-test for testing:\n",
    "\n",
    "- the null hypothesis $H_{0} : \\beta_{1}=0$\n",
    "- against the alternative hypothesis $H_{\\mathrm{A}} : \\beta_{1} \\neq 0$\n",
    "\n",
    "the P-value of the t-test for \"Lat\" is less than 0.001. \n",
    "\n",
    "There is enough statistical evidence to conclude that the slope is not 0, that is, that there is a linear relationship between skin cancer mortality and latitude.\n",
    "\n",
    "There is an alternative method for answering the research question, which uses the analysis of variance F-test. Let's first look at what we are working towards understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "R1N1BKmMQ8yN"
   },
   "source": [
    "## Analysis of Variance\n",
    "\n",
    "Below, there is a column labeled F, which contains the F-test statistic, and there is a column labeled P, which contains the P-value associated with the F-test\n",
    "\n",
    "Notice that the P-value, 0.000, appears to be the same as the P-value, 0.000, for the t-test for the slope.\n",
    "\n",
    "The F-test similarly tells us that there is enough statistical evidence to conclude that there is a linear relationship between skin cancer mortality and latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1556359853146,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "mExhjtqQQCqC",
    "outputId": "2cb82b37-6cbe-43cf-bbba-65b384551c32"
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "mod = smf.ols(formula='Mort ~ Lat', data=df)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "4WKmVXj2SiPR"
   },
   "source": [
    "let's investigate the components of the Analysis of variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "Oinv4-dLSqd-"
   },
   "source": [
    "### Sums of Squares\n",
    "\n",
    "We considered sums of squares in lesson about [OLS](https://dynalist.io/d/Fm5tz2wS5-UilvDNXmLSOyv9) when we defined the coefficient of determination, \n",
    "r\n",
    "2\n",
    ", but now we consider them again in the context of the analysis of variance table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1556360330280,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "xlq9Xx3JS78z",
    "outputId": "e9c0c35a-4185-4139-931e-846a8bb96f81"
   },
   "outputs": [],
   "source": [
    "interc = df['Mort'].mean()\n",
    "\n",
    "data_x = [30, 35, 30]\n",
    "data_y = [209.86, 141,interc\n",
    "         ]\n",
    "\n",
    "labels = [r'$\\hat{y}_{i}=389.19-5.98 x_{i}$',\n",
    "          r'$y_{i}$', r'$\\overline{y}=152.88$'\n",
    "         ]\n",
    "\n",
    "plt.plot(x, y, 'o', label='original data')\n",
    "plt.axhline(interc, label='Reducel model', color = 'g')\n",
    "plt.plot(x, intercept + slope * x, 'r', label='Full model')\n",
    "\n",
    "\n",
    "for label, x_1, y_2 in zip(labels, data_x, data_y):\n",
    "  plt.annotate(label,\n",
    "            xy=(x_1, y_2), xycoords='data',\n",
    "            xytext=(-45, -50),\n",
    "            textcoords='offset points',\n",
    "            arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Mortality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "AuplLABHTi8p"
   },
   "source": [
    "The scatter plot of mortality and latitude appears again below, but now it is adorned with three labels:\n",
    "\n",
    "- $y_{i}$ denotes the observed mortality for state i\n",
    "- $\\hat{y}_{i}$ is the estimated regression line (red line) and therefore denotes the estimated (or \"fitted\") mortality for the latitude of state i\n",
    "- $\\overline{y}$ represents what the line would look like if there were no relationship between mortality and latitude. That is, it denotes the \"no relationship\" line (green line). It is simply the average mortality of the sample.\n",
    "\n",
    "If there is a linear relationship between mortality and latitude, then the estimated regression line should be \"far\" from the no relationship line. \n",
    "\n",
    "We just need a way of quantifying \"far.\" The above three elements are useful in quantifying how far the estimated regression line is from the no relationship line. \n",
    "\n",
    "As illustrated by the plot, the two lines are quite far apart.\n",
    "\n",
    "The distance of each observed value $y_i$ from the no regression line $y_{i}-\\overline{y}$\n",
    ". If you determine this distance for each data point, square each distance, and add up all of the squared distances, you get:\n",
    "\n",
    "$$\\sum_{i=1}^{n}\\left(y_{i}-\\overline{y}\\right)^{2}=53637$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1556360645071,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "w7q9fF-3UJJr",
    "outputId": "b8feeb35-896e-427c-8855-e03a50466086"
   },
   "outputs": [],
   "source": [
    "TSS = np.sum(np.power((y - interc), 2))\n",
    "TSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ycRm6ZR4UNY2"
   },
   "source": [
    "Called the \"total sum of squares,\" it quantifies how much the observed responses vary if you don't take into account their latitude.\n",
    "\n",
    "The distance of each fitted value $\\hat{y}_{i}$ from the no regression line $\\overline{y}$ is$\\hat{y}_{i}-\\overline{y}$. If you determine this distance for each data point, square each distance, and add up all of the squared distances, you get:\n",
    "\n",
    "$\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\overline{y}\\right)^{2}=36464$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1556360647263,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "Baa_vg_CUheN",
    "outputId": "6d8b50a7-4745-463a-9b09-a8a748ba24c6"
   },
   "outputs": [],
   "source": [
    "y_hat  = intercept + slope * x\n",
    "RSS = np.sum(np.power((y_hat - interc), 2))\n",
    "RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "NQcWGMg2UtvU"
   },
   "source": [
    "Called the \"regression sum of squares,\" it quantifies how far the estimated regression line is from the no relationship line.\n",
    "\n",
    "The distance of each observed value $y_{i}$ from the estimated regression line $\\hat{y}_{i}$ is $y_{i}-\\hat{y}_{i}$ If you determine this distance for each data point, square each distance, and add up all of the squared distances, you get:\n",
    "\n",
    "$$\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=17173$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1556360728277,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "TDPXbUKoU9Yt",
    "outputId": "892cc8a6-9dfb-47a0-b201-329570fb4b77"
   },
   "outputs": [],
   "source": [
    "y_hat  = intercept + slope * x\n",
    "ESS = np.sum(np.power((y - y_hat), 2))\n",
    "ESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "wxzrVQ4RVCzo"
   },
   "source": [
    "Called the \"error sum of squares,\" as you know, it quantifies how much the data points vary around the estimated regression line.\n",
    "\n",
    "In short, we have illustrated that the total variation in observed mortality y (53637) is the sum of two parts — variation \"due to\" latitude (36464) and variation just due to random error (17173). \n",
    "\n",
    "We are careful to put \"due to\" in quotes in order to emphasize that a change in latitude does not necessarily cause the change in mortality. All we could conclude is that latitude is \"associated with\" mortality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "-YRIHLPNaIjY"
   },
   "source": [
    "## Interpretation\n",
    "\n",
    "Recall, we have 49 states in the data set:\n",
    "\n",
    "- The degrees of freedom associated with SSR will always be 1 for the simple linear regression model. \n",
    "- The degrees of freedom associated with SSTO is n-1 = 49-1 = 48. \n",
    "- The degrees of freedom associated with SSE is n-2 = 49-2 = 47. And the degrees of freedom add up: 1 + 47 = 48.\n",
    "- The sums of squares add up: SSTO = SSR + SSE. That is, here: 53637 = 36464 + 17173.\n",
    "\n",
    "Let's tackle a few more columns of the analysis of variance table, namely the \"mean square\" column, labled MS, and the F-statistic column, labeled F\n",
    "\n",
    "Although the derivation isn't as simple as it seems, the decomposition holds for the sum of the squared distances, too:\n",
    "$$\\sum_{i=1}^{n}\\left(y_{i}-\\overline{y}\\right)^{2}=\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\overline{y}\\right)^{2}+\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$$\n",
    "\n",
    "SSTO = SSE+ SSR\n",
    "\n",
    "![Variance_decomposition.png](https://dynalist.io/u/Ljf0NOwEdcOPrHaYUZ4KeLh9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "Nvrf2Cwja6cV"
   },
   "source": [
    "### Mean squares\n",
    "\n",
    "We already know the \"mean square error (MSE)\" is defined as:\n",
    "\n",
    "$$M S E=\\frac{\\sum\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}{n-2}=\\frac{S S E}{n-2}$$\n",
    "\n",
    "That is, we obtain the mean square error by dividing the error sum of squares by its associated degrees of freedom n-2\n",
    "\n",
    "Similarly, we obtain the \"regression mean square (MSR)\" by dividing the regression sum of squares by its degrees of freedom 1:\n",
    "\n",
    "$$M S R=\\frac{\\sum\\left(\\hat{y}_{i}-\\overline{y}\\right)^{2}}{1}=\\frac{S S R}{1}$$\n",
    "\n",
    "Of course, that means the regression sum of squares (SSR) and the regression mean square (MSR) are always identical for the simple linear regression model\n",
    "\n",
    "Now, why do we care about mean squares? \n",
    "\n",
    "Because their expected values suggest how to test the null hypothesis $H_{0} : \\beta_{1}=0$ against the alternative hypothesis $H_{\\mathrm{A}} : \\beta_{1} \\neq 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "B-g1gtMgbsCj"
   },
   "source": [
    "### Expected Mean Squares\n",
    "\n",
    "Imagine taking many, many random samples of size n from some population, and estimating the regression line and determining MSR and MSE for each data set obtained. It has been shown that the average (that is, the expected value) of all of the MSRs you can obtain equals\n",
    "\n",
    "$$E(M S R)=\\sigma^{2}+\\beta_{1}^{2} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}$$\n",
    "\n",
    "Similarly, it has been shown that the average (that is, the expected value) of all of the MSEs you can obtain equals:\n",
    "\n",
    "$$E(M S E)=\\sigma^{2}$$\n",
    "\n",
    "  These expected values suggest how to test $$H_{0} : \\beta_{1}=0$$ versus $$H_{\\mathrm{A}} : \\beta_{1} \\neq $$\n",
    "  \n",
    "- If $\\beta_{1}=0$, then we'd expect the ratio MSR/MSE to equal 1.\n",
    "- If $\\beta_{1} \\neq 0$, then we'd expect the ratio MSR/MSE to be greater than 1\n",
    "\n",
    "These two facts suggest that we should use the ratio, MSR/MSE, to determine whether or not $\\beta_{1} = 0$\n",
    "\n",
    "Note that, because $\\beta_1$ is squared in E(MSR), we cannot use the ratio MSR/MSE:\n",
    "\n",
    "- to test $H_{0} : \\beta_{1}=0$ versus $H_{\\mathrm{A}} : \\beta_{1}<0$\n",
    "- or to test $\\mathrm{H}_{0} : \\beta_{1}=0$ versus $\\mathrm{H}_{\\mathrm{A}} : \\beta_{1}>0$\n",
    "\n",
    "We can only use MSR/MSE to test $H_{0} : \\beta_{1}=0$ versus $H_{\\mathrm{A}} : \\beta_{1} \\neq 0$\n",
    "\n",
    "We have now completed our investigation of all of the entries of a standard analysis of variance table. The formula for each entry is summarized for you in the following analysis of variance table:\n",
    "\n",
    "| Source of Variation | DF  | SS                                                              | MS                        | F                           |\n",
    "|---------------------|-----|-----------------------------------------------------------------|---------------------------|-----------------------------|\n",
    "| Regression          | 1   | $$S S R=\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\overline{y}\\right)^{2}$$ | $$M S R=\\frac{S S R}{1}$$   | $$F^{*}=\\frac{M S R}{M S E}$$ |\n",
    "| Residual error      | n-2 | $$S S E=\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$$        | $$M S E=\\frac{S S E}{n-2}$$ |                             |\n",
    "| Total               | n-1 | $$S S T O=\\sum_{i=1}^{n}\\left(y_{i}-\\overline{y}\\right)^{2}$$     |                           |                             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "_wRHjvBaeSzb"
   },
   "source": [
    "## The formal F-test for the slope parameter $\\beta_1$\n",
    "\n",
    "The null hypothesis is $H_{0} : \\beta_{1}=0$\n",
    "The alternative hypothesis is  $H_{\\mathrm{A}} : \\beta_{1} \\neq 0$\n",
    "The test statistic is  $F^{*}=\\frac{M S R}{M S E}$\n",
    "\n",
    "As always, the P-value is obtained by answering the question: \"What is the probability that we’d get an F* statistic as large as we did, if the null hypothesis is true?\n",
    "\n",
    "The P-value is determined by comparing F* to an F distribution with 1 numerator degree of freedom and n-2 denominator degrees of freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "exBAZ7Dbew8-"
   },
   "source": [
    "## Example \n",
    "\n",
    "The following data set contains the winning times (in seconds) of the 22 men's 200 meter olympic sprints held between 1900 and 1996\n",
    "\n",
    "Is there a linear relationship between year and the winning times? The plot of the estimated regression line sure makes it look so!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "vquPYQjOfYOI"
   },
   "outputs": [],
   "source": [
    "range_name = 'mens200m.csv!A2:B23'\n",
    "headers = ['Year', 'Men200']\n",
    "\n",
    "df = service['sheet'].spreadsheets().values().get(\n",
    "    spreadsheetId='1YVEf9PuU5NHcTqqF3qFrVXtmxWWnxuanmL2Xx9UQClo',\n",
    "    range=range_name).execute()\n",
    "df = pd.DataFrame(df.get('values', []), columns=headers)\n",
    "df = df.apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1556363899431,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "4fXXlG7sfk6u",
    "outputId": "93d70e47-e794-4f06-d9b2-2bf439f0faf9"
   },
   "outputs": [],
   "source": [
    "y = df['Men200']\n",
    "x = df['Year']\n",
    "\n",
    "interc = df['Men200'].mean()\n",
    "\n",
    "#data_x = [30, 35]\n",
    "#data_y = [209.86, 141]\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "print(\"slope: %f    intercept: %f\" % (slope, intercept))\n",
    "print(\"p-values: %f\" % (p_value))\n",
    "print(\"r-squared: %f\" % r_value**2)\n",
    "  \n",
    "#### Plot with label\n",
    "labels = [r'$\\mu_{Y} = E(Y)=\\beta_{0}+\\beta_{1} X$',\n",
    "          r'$Y_{i}= \\beta_{0}+\\beta_{1} X+\\varepsilon_{i}$']\n",
    "\n",
    "plt.plot(x, y, 'o', label='original data')\n",
    "plt.plot(x, intercept + slope * x, 'r', label='fitted line')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean 200')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ZGj0stMPgMLi"
   },
   "source": [
    "To answer the research question, let's conduct the formal F-test of the null hypothesis $H_{0} : \\beta_{1}=0$ against the alternative hypothesis $H_{\\mathrm{A}} : \\beta_{1} \\neq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "iy3YXG7EgUhY"
   },
   "source": [
    "### Analyse of Variance\n",
    "\n",
    "From a scientific point of view, what we ultimately care about is the P-value.\n",
    "\n",
    "That is, the P-value is less than 0.001. The P-value is very small. It is unlikely that we would have obtained such a large F* statistic if the null hypothesis were true. \n",
    "\n",
    "Therefore, we reject the null hypothesis $H_{0} : \\beta_{1}=0$ in favor of the alternative hypothesis $H_{\\mathrm{A}} : \\beta_{1} \\neq 0$.\n",
    "\n",
    "There is sufficient evidence at the $\\alpha=0.05$ level to conclude that there is a linear relationship between year and winning time\n",
    "\n",
    "### Equivalence of the analysis of variance F-test and the t-test\n",
    "\n",
    "As we noted in the first two examples, the P-value associated with the t-test is the same as the P-value associated with the analysis of variance F-test.\n",
    "\n",
    "This will always be true for the simple linear regression model\n",
    "\n",
    "The F-test is more useful for the multiple regression model when we want to test that more than one slope parameter is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1556364527127,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "61ebeIF9hRU-",
    "outputId": "1c7f1994-1532-4804-f1c0-0fb736f919ad"
   },
   "outputs": [],
   "source": [
    "TSS = np.sum(np.power((y - interc), 2))\n",
    "y_hat  = intercept + slope * x\n",
    "RSS = np.sum(np.power((y_hat - interc), 2))\n",
    "ESS = np.sum(np.power((y - y_hat), 2))\n",
    "n = len(df)\n",
    "\n",
    "ANOVA = {\n",
    "    'Regression':{\n",
    "        'DF': 1,\n",
    "        'SS': RSS,\n",
    "        'MS': RSS/1,\n",
    "        'F': RSS/ (ESS/(n - 2)),\n",
    "        'P': 0.000\n",
    "    },\n",
    "    'Residual_error':{\n",
    "        'DF': n - 2,\n",
    "        'SS': ESS,\n",
    "        'MS': ESS/(n - 2)\n",
    "    },\n",
    "    'Total':{\n",
    "        'DF': n - 1,\n",
    "        'SS': TSS\n",
    "    }\n",
    "}\n",
    "pd.DataFrame(ANOVA).T.fillna('')[['DF', 'SS', 'MS', 'F', 'P']]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "YEDKjsw7P1SY"
   ],
   "name": "03_Sum_Squares.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
