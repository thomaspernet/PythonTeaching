{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "V5lIB3vzPkL5"
   },
   "source": [
    "# Sums of squares\n",
    "\n",
    "Source: [PennState Stat 501](https://newonlinecourses.science.psu.edu/stat501/node/263/)\n",
    "\n",
    "\"Is there a (linear) relationship between skin cancer mortality and latitude?\"\n",
    "\n",
    "Review the following scatter plot and estimated regression line. What does the plot suggest is the answer to the research question? The linear relationship looks fairly strong. The estimated slope is negative, not equal to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "YEDKjsw7P1SY"
   },
   "source": [
    "## Upload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3719,
     "status": "ok",
     "timestamp": 1556359388172,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "p7ksZMTnPfs4",
    "outputId": "ca698179-f524-4a68-aadc-39394682bbbc"
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 31 fields in line 2, saw 33\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7aac5f2bcf7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mrange_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'skin_cancer!A2:E50'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://online.stat.psu.edu/onlinecourses/sites/stat501/files/data/skincancer.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2035\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2037\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2038\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 31 fields in line 2, saw 33\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "headers = ['State','Lat','Mort','Ocean','Long']\n",
    "range_name = 'skin_cancer!A2:E50'\n",
    "\n",
    "df = pd.read_csv('https://online.stat.psu.edu/onlinecourses/sites/stat501/files/data/skincancer.txt',sep=\" \", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "7pkfv0G1P6kH"
   },
   "source": [
    "## Fitted values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 251,
     "status": "ok",
     "timestamp": 1556359607076,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "O8Q7LHroP-2x",
    "outputId": "b4971847-1515-46b8-8c81-ac1b7a183ac1"
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = df['Mort']\n",
    "x = df['Lat']\n",
    "\n",
    "#data_x = [30, 35]\n",
    "#data_y = [209.86, 141]\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "print(\"slope: %f    intercept: %f\" % (slope, intercept))\n",
    "print(\"p-values: %f\" % (p_value))\n",
    "print(\"r-squared: %f\" % r_value**2)\n",
    "  \n",
    "#### Plot with label\n",
    "labels = [r'$\\mu_{Y} = E(Y)=\\beta_{0}+\\beta_{1} X$',\n",
    "          r'$Y_{i}= \\beta_{0}+\\beta_{1} X+\\varepsilon_{i}$']\n",
    "\n",
    "plt.plot(x, y, 'o', label='original data')\n",
    "plt.plot(x, intercept + slope * x, 'r', label='fitted line')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Mortality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "0n3StuZMQL6e"
   },
   "source": [
    "## Research Question\n",
    "\n",
    "We can answer the research question using the P-value of the t-test for testing:\n",
    "\n",
    "- the null hypothesis $H_{0} : \\beta_{1}=0$\n",
    "- against the alternative hypothesis $H_{\\mathrm{A}} : \\beta_{1} \\neq 0$\n",
    "\n",
    "the P-value of the t-test for \"Lat\" is less than 0.001. \n",
    "\n",
    "There is enough statistical evidence to conclude that the slope is not 0, that is, that there is a linear relationship between skin cancer mortality and latitude.\n",
    "\n",
    "There is an alternative method for answering the research question, which uses the analysis of variance F-test. Let's first look at what we are working towards understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "R1N1BKmMQ8yN"
   },
   "source": [
    "## Analysis of Variance\n",
    "\n",
    "Below, there is a column labeled F, which contains the F-test statistic, and there is a column labeled P, which contains the P-value associated with the F-test\n",
    "\n",
    "Notice that the P-value, 0.000, appears to be the same as the P-value, 0.000, for the t-test for the slope.\n",
    "\n",
    "The F-test similarly tells us that there is enough statistical evidence to conclude that there is a linear relationship between skin cancer mortality and latitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 108,
     "status": "ok",
     "timestamp": 1556359853146,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "mExhjtqQQCqC",
    "outputId": "2cb82b37-6cbe-43cf-bbba-65b384551c32"
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "mod = smf.ols(formula='Mort ~ Lat', data=df)\n",
    "res = mod.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "4WKmVXj2SiPR"
   },
   "source": [
    "let's investigate the components of the Analysis of variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "Oinv4-dLSqd-"
   },
   "source": [
    "### Sums of Squares\n",
    "\n",
    "We considered sums of squares in lesson about [OLS](https://dynalist.io/d/Fm5tz2wS5-UilvDNXmLSOyv9) when we defined the coefficient of determination, \n",
    "r\n",
    "2\n",
    ", but now we consider them again in the context of the analysis of variance table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 388,
     "status": "ok",
     "timestamp": 1556360330280,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "xlq9Xx3JS78z",
    "outputId": "e9c0c35a-4185-4139-931e-846a8bb96f81"
   },
   "outputs": [],
   "source": [
    "interc = df['Mort'].mean()\n",
    "\n",
    "data_x = [30, 35, 30]\n",
    "data_y = [209.86, 141,interc\n",
    "         ]\n",
    "\n",
    "labels = [r'$\\hat{y}_{i}=389.19-5.98 x_{i}$',\n",
    "          r'$y_{i}$', r'$\\overline{y}=152.88$'\n",
    "         ]\n",
    "\n",
    "plt.plot(x, y, 'o', label='original data')\n",
    "plt.axhline(interc, label='Reducel model', color = 'g')\n",
    "plt.plot(x, intercept + slope * x, 'r', label='Full model')\n",
    "\n",
    "\n",
    "for label, x_1, y_2 in zip(labels, data_x, data_y):\n",
    "  plt.annotate(label,\n",
    "            xy=(x_1, y_2), xycoords='data',\n",
    "            xytext=(-45, -50),\n",
    "            textcoords='offset points',\n",
    "            arrowprops=dict(arrowstyle=\"->\"))\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Latitude')\n",
    "plt.ylabel('Mortality')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "AuplLABHTi8p"
   },
   "source": [
    "The scatter plot of mortality and latitude appears again below, but now it is adorned with three labels:\n",
    "\n",
    "- $y_{i}$ denotes the observed mortality for state i\n",
    "- $\\hat{y}_{i}$ is the estimated regression line (red line) and therefore denotes the estimated (or \"fitted\") mortality for the latitude of state i\n",
    "- $\\overline{y}$ represents what the line would look like if there were no relationship between mortality and latitude. That is, it denotes the \"no relationship\" line (green line). It is simply the average mortality of the sample.\n",
    "\n",
    "If there is a linear relationship between mortality and latitude, then the estimated regression line should be \"far\" from the no relationship line. \n",
    "\n",
    "We just need a way of quantifying \"far.\" The above three elements are useful in quantifying how far the estimated regression line is from the no relationship line. \n",
    "\n",
    "As illustrated by the plot, the two lines are quite far apart.\n",
    "\n",
    "The distance of each observed value $y_i$ from the no regression line $y_{i}-\\overline{y}$\n",
    ". If you determine this distance for each data point, square each distance, and add up all of the squared distances, you get:\n",
    "\n",
    "$$\\sum_{i=1}^{n}\\left(y_{i}-\\overline{y}\\right)^{2}=53637$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1556360645071,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "w7q9fF-3UJJr",
    "outputId": "b8feeb35-896e-427c-8855-e03a50466086"
   },
   "outputs": [],
   "source": [
    "TSS = np.sum(np.power((y - interc), 2))\n",
    "TSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ycRm6ZR4UNY2"
   },
   "source": [
    "Called the \"total sum of squares,\" it quantifies how much the observed responses vary if you don't take into account their latitude.\n",
    "\n",
    "The distance of each fitted value $\\hat{y}_{i}$ from the no regression line $\\overline{y}$ is$\\hat{y}_{i}-\\overline{y}$. If you determine this distance for each data point, square each distance, and add up all of the squared distances, you get:\n",
    "\n",
    "$\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\overline{y}\\right)^{2}=36464$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1556360647263,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "Baa_vg_CUheN",
    "outputId": "6d8b50a7-4745-463a-9b09-a8a748ba24c6"
   },
   "outputs": [],
   "source": [
    "y_hat  = intercept + slope * x\n",
    "RSS = np.sum(np.power((y_hat - interc), 2))\n",
    "RSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "NQcWGMg2UtvU"
   },
   "source": [
    "Called the \"regression sum of squares,\" it quantifies how far the estimated regression line is from the no relationship line.\n",
    "\n",
    "The distance of each observed value $y_{i}$ from the estimated regression line $\\hat{y}_{i}$ is $y_{i}-\\hat{y}_{i}$ If you determine this distance for each data point, square each distance, and add up all of the squared distances, you get:\n",
    "\n",
    "$$\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}=17173$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1556360728277,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "TDPXbUKoU9Yt",
    "outputId": "892cc8a6-9dfb-47a0-b201-329570fb4b77"
   },
   "outputs": [],
   "source": [
    "y_hat  = intercept + slope * x\n",
    "ESS = np.sum(np.power((y - y_hat), 2))\n",
    "ESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "wxzrVQ4RVCzo"
   },
   "source": [
    "Called the \"error sum of squares,\" as you know, it quantifies how much the data points vary around the estimated regression line.\n",
    "\n",
    "In short, we have illustrated that the total variation in observed mortality y (53637) is the sum of two parts — variation \"due to\" latitude (36464) and variation just due to random error (17173). \n",
    "\n",
    "We are careful to put \"due to\" in quotes in order to emphasize that a change in latitude does not necessarily cause the change in mortality. All we could conclude is that latitude is \"associated with\" mortality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "-YRIHLPNaIjY"
   },
   "source": [
    "## Interpretation\n",
    "\n",
    "Recall, we have 49 states in the data set:\n",
    "\n",
    "- The degrees of freedom associated with SSR will always be 1 for the simple linear regression model. \n",
    "- The degrees of freedom associated with SSTO is n-1 = 49-1 = 48. \n",
    "- The degrees of freedom associated with SSE is n-2 = 49-2 = 47. And the degrees of freedom add up: 1 + 47 = 48.\n",
    "- The sums of squares add up: SSTO = SSR + SSE. That is, here: 53637 = 36464 + 17173.\n",
    "\n",
    "Let's tackle a few more columns of the analysis of variance table, namely the \"mean square\" column, labled MS, and the F-statistic column, labeled F\n",
    "\n",
    "Although the derivation isn't as simple as it seems, the decomposition holds for the sum of the squared distances, too:\n",
    "$$\\sum_{i=1}^{n}\\left(y_{i}-\\overline{y}\\right)^{2}=\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\overline{y}\\right)^{2}+\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$$\n",
    "\n",
    "SSTO = SSE+ SSR\n",
    "\n",
    "![Variance_decomposition.png](https://dynalist.io/u/Ljf0NOwEdcOPrHaYUZ4KeLh9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "Nvrf2Cwja6cV"
   },
   "source": [
    "### Mean squares\n",
    "\n",
    "We already know the \"mean square error (MSE)\" is defined as:\n",
    "\n",
    "$$M S E=\\frac{\\sum\\left(y_{i}-\\hat{y}_{i}\\right)^{2}}{n-2}=\\frac{S S E}{n-2}$$\n",
    "\n",
    "That is, we obtain the mean square error by dividing the error sum of squares by its associated degrees of freedom n-2\n",
    "\n",
    "Similarly, we obtain the \"regression mean square (MSR)\" by dividing the regression sum of squares by its degrees of freedom 1:\n",
    "\n",
    "$$M S R=\\frac{\\sum\\left(\\hat{y}_{i}-\\overline{y}\\right)^{2}}{1}=\\frac{S S R}{1}$$\n",
    "\n",
    "Of course, that means the regression sum of squares (SSR) and the regression mean square (MSR) are always identical for the simple linear regression model\n",
    "\n",
    "Now, why do we care about mean squares? \n",
    "\n",
    "Because their expected values suggest how to test the null hypothesis $H_{0} : \\beta_{1}=0$ against the alternative hypothesis $H_{\\mathrm{A}} : \\beta_{1} \\neq 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "B-g1gtMgbsCj"
   },
   "source": [
    "### Expected Mean Squares\n",
    "\n",
    "Imagine taking many, many random samples of size n from some population, and estimating the regression line and determining MSR and MSE for each data set obtained. It has been shown that the average (that is, the expected value) of all of the MSRs you can obtain equals\n",
    "\n",
    "$$E(M S R)=\\sigma^{2}+\\beta_{1}^{2} \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}$$\n",
    "\n",
    "Similarly, it has been shown that the average (that is, the expected value) of all of the MSEs you can obtain equals:\n",
    "\n",
    "$$E(M S E)=\\sigma^{2}$$\n",
    "\n",
    "  These expected values suggest how to test $$H_{0} : \\beta_{1}=0$$ versus $$H_{\\mathrm{A}} : \\beta_{1} \\neq $$\n",
    "  \n",
    "- If $\\beta_{1}=0$, then we'd expect the ratio MSR/MSE to equal 1.\n",
    "- If $\\beta_{1} \\neq 0$, then we'd expect the ratio MSR/MSE to be greater than 1\n",
    "\n",
    "These two facts suggest that we should use the ratio, MSR/MSE, to determine whether or not $\\beta_{1} = 0$\n",
    "\n",
    "Note that, because $\\beta_1$ is squared in E(MSR), we cannot use the ratio MSR/MSE:\n",
    "\n",
    "- to test $H_{0} : \\beta_{1}=0$ versus $H_{\\mathrm{A}} : \\beta_{1}<0$\n",
    "- or to test $\\mathrm{H}_{0} : \\beta_{1}=0$ versus $\\mathrm{H}_{\\mathrm{A}} : \\beta_{1}>0$\n",
    "\n",
    "We can only use MSR/MSE to test $H_{0} : \\beta_{1}=0$ versus $H_{\\mathrm{A}} : \\beta_{1} \\neq 0$\n",
    "\n",
    "We have now completed our investigation of all of the entries of a standard analysis of variance table. The formula for each entry is summarized for you in the following analysis of variance table:\n",
    "\n",
    "| Source of Variation | DF  | SS                                                              | MS                        | F                           |\n",
    "|---------------------|-----|-----------------------------------------------------------------|---------------------------|-----------------------------|\n",
    "| Regression          | 1   | $$S S R=\\sum_{i=1}^{n}\\left(\\hat{y}_{i}-\\overline{y}\\right)^{2}$$ | $$M S R=\\frac{S S R}{1}$$   | $$F^{*}=\\frac{M S R}{M S E}$$ |\n",
    "| Residual error      | n-2 | $$S S E=\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$$        | $$M S E=\\frac{S S E}{n-2}$$ |                             |\n",
    "| Total               | n-1 | $$S S T O=\\sum_{i=1}^{n}\\left(y_{i}-\\overline{y}\\right)^{2}$$     |                           |                             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "_wRHjvBaeSzb"
   },
   "source": [
    "## The formal F-test for the slope parameter $\\beta_1$\n",
    "\n",
    "The null hypothesis is $H_{0} : \\beta_{1}=0$\n",
    "The alternative hypothesis is  $H_{\\mathrm{A}} : \\beta_{1} \\neq 0$\n",
    "The test statistic is  $F^{*}=\\frac{M S R}{M S E}$\n",
    "\n",
    "As always, the P-value is obtained by answering the question: \"What is the probability that we’d get an F* statistic as large as we did, if the null hypothesis is true?\n",
    "\n",
    "The P-value is determined by comparing F* to an F distribution with 1 numerator degree of freedom and n-2 denominator degrees of freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "exBAZ7Dbew8-"
   },
   "source": [
    "## Example \n",
    "\n",
    "The following data set contains the winning times (in seconds) of the 22 men's 200 meter olympic sprints held between 1900 and 1996\n",
    "\n",
    "Is there a linear relationship between year and the winning times? The plot of the estimated regression line sure makes it look so!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {},
    "colab_type": "code",
    "id": "vquPYQjOfYOI"
   },
   "outputs": [],
   "source": [
    "range_name = 'mens200m.csv!A2:B23'\n",
    "headers = ['Year', 'Men200']\n",
    "\n",
    "df = service['sheet'].spreadsheets().values().get(\n",
    "    spreadsheetId='1YVEf9PuU5NHcTqqF3qFrVXtmxWWnxuanmL2Xx9UQClo',\n",
    "    range=range_name).execute()\n",
    "df = pd.DataFrame(df.get('values', []), columns=headers)\n",
    "df = df.apply(pd.to_numeric, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1556363899431,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "4fXXlG7sfk6u",
    "outputId": "93d70e47-e794-4f06-d9b2-2bf439f0faf9"
   },
   "outputs": [],
   "source": [
    "y = df['Men200']\n",
    "x = df['Year']\n",
    "\n",
    "interc = df['Men200'].mean()\n",
    "\n",
    "#data_x = [30, 35]\n",
    "#data_y = [209.86, 141]\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "print(\"slope: %f    intercept: %f\" % (slope, intercept))\n",
    "print(\"p-values: %f\" % (p_value))\n",
    "print(\"r-squared: %f\" % r_value**2)\n",
    "  \n",
    "#### Plot with label\n",
    "labels = [r'$\\mu_{Y} = E(Y)=\\beta_{0}+\\beta_{1} X$',\n",
    "          r'$Y_{i}= \\beta_{0}+\\beta_{1} X+\\varepsilon_{i}$']\n",
    "\n",
    "plt.plot(x, y, 'o', label='original data')\n",
    "plt.plot(x, intercept + slope * x, 'r', label='fitted line')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean 200')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "ZGj0stMPgMLi"
   },
   "source": [
    "To answer the research question, let's conduct the formal F-test of the null hypothesis $H_{0} : \\beta_{1}=0$ against the alternative hypothesis $H_{\\mathrm{A}} : \\beta_{1} \\neq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "colab_type": "text",
    "id": "iy3YXG7EgUhY"
   },
   "source": [
    "### Analyse of Variance\n",
    "\n",
    "From a scientific point of view, what we ultimately care about is the P-value.\n",
    "\n",
    "That is, the P-value is less than 0.001. The P-value is very small. It is unlikely that we would have obtained such a large F* statistic if the null hypothesis were true. \n",
    "\n",
    "Therefore, we reject the null hypothesis $H_{0} : \\beta_{1}=0$ in favor of the alternative hypothesis $H_{\\mathrm{A}} : \\beta_{1} \\neq 0$.\n",
    "\n",
    "There is sufficient evidence at the $\\alpha=0.05$ level to conclude that there is a linear relationship between year and winning time\n",
    "\n",
    "### Equivalence of the analysis of variance F-test and the t-test\n",
    "\n",
    "As we noted in the first two examples, the P-value associated with the t-test is the same as the P-value associated with the analysis of variance F-test.\n",
    "\n",
    "This will always be true for the simple linear regression model\n",
    "\n",
    "The F-test is more useful for the multiple regression model when we want to test that more than one slope parameter is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1556364527127,
     "user": {
      "displayName": "Thomas Pernet",
      "photoUrl": "https://lh5.googleusercontent.com/-MpE5ChsC2J4/AAAAAAAAAAI/AAAAAAAABsw/7sBoF1HatSI/s64/photo.jpg",
      "userId": "12184844431429654860"
     },
     "user_tz": -120
    },
    "id": "61ebeIF9hRU-",
    "outputId": "1c7f1994-1532-4804-f1c0-0fb736f919ad"
   },
   "outputs": [],
   "source": [
    "TSS = np.sum(np.power((y - interc), 2))\n",
    "y_hat  = intercept + slope * x\n",
    "RSS = np.sum(np.power((y_hat - interc), 2))\n",
    "ESS = np.sum(np.power((y - y_hat), 2))\n",
    "n = len(df)\n",
    "\n",
    "ANOVA = {\n",
    "    'Regression':{\n",
    "        'DF': 1,\n",
    "        'SS': RSS,\n",
    "        'MS': RSS/1,\n",
    "        'F': RSS/ (ESS/(n - 2)),\n",
    "        'P': 0.000\n",
    "    },\n",
    "    'Residual_error':{\n",
    "        'DF': n - 2,\n",
    "        'SS': ESS,\n",
    "        'MS': ESS/(n - 2)\n",
    "    },\n",
    "    'Total':{\n",
    "        'DF': n - 1,\n",
    "        'SS': TSS\n",
    "    }\n",
    "}\n",
    "pd.DataFrame(ANOVA).T.fillna('')[['DF', 'SS', 'MS', 'F', 'P']]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "YEDKjsw7P1SY"
   ],
   "name": "03_Sum_Squares.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
